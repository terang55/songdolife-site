"""
ê°œì„ ëœ í¬ë¡¤ë§ ë°ì´í„° í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import os
from datetime import datetime

def check_enhanced_data():
    """ê°œì„ ëœ í¬ë¡¤ë§ ë°ì´í„° í™•ì¸"""
    print("=" * 80)
    print("ğŸ” ê°œì„ ëœ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„")
    print("=" * 80)
    
    enhanced_dir = "../data/enhanced_news"
    
    if not os.path.exists(enhanced_dir):
        print("âŒ enhanced_news ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    files = [f for f in os.listdir(enhanced_dir) if f.endswith('.json')]
    
    if not files:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(files)}ê°œ")
    print()
    
    total_articles = 0
    
    for file in files:
        file_path = os.path.join(enhanced_dir, file)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“„ íŒŒì¼: {file}")
            print(f"   ğŸ“Š ê¸°ì‚¬ ìˆ˜: {len(data)}ê°œ")
            
            if data:
                total_articles += len(data)
                
                # ì²« ë²ˆì§¸ ê¸°ì‚¬ ì •ë³´ ì¶œë ¥
                first_article = data[0]
                print(f"   ğŸ“° ì²« ë²ˆì§¸ ê¸°ì‚¬:")
                print(f"      ì œëª©: {first_article.get('title', 'ì œëª© ì—†ìŒ')[:80]}...")
                print(f"      ì–¸ë¡ ì‚¬: {first_article.get('press', 'ì–¸ë¡ ì‚¬ ì—†ìŒ')}")
                print(f"      ë‚ ì§œ: {first_article.get('date', 'ë‚ ì§œ ì—†ìŒ')}")
                print(f"      ë‚´ìš© ê¸¸ì´: {first_article.get('content_length', 0)}ì")
                print(f"      í‚¤ì›Œë“œ: {first_article.get('keyword', 'í‚¤ì›Œë“œ ì—†ìŒ')}")
                print(f"      URL: {first_article.get('url', 'URL ì—†ìŒ')[:100]}...")
                
                # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                content = first_article.get('content', '')
                if content:
                    preview = content[:200] + "..." if len(content) > 200 else content
                    print(f"      ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {preview}")
                
                print()
                
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file}): {str(e)}")
    
    print("=" * 80)
    print(f"ğŸ“Š ì „ì²´ ìš”ì•½:")
    print(f"   ì´ íŒŒì¼ ìˆ˜: {len(files)}ê°œ")
    print(f"   ì´ ê¸°ì‚¬ ìˆ˜: {total_articles}ê°œ")
    print("=" * 80)

def check_summary():
    """í¬ë¡¤ë§ ìš”ì•½ íŒŒì¼ í™•ì¸"""
    print("\nğŸ“‹ í¬ë¡¤ë§ ìš”ì•½ í™•ì¸:")
    
    data_dir = "../data"
    summary_files = [f for f in os.listdir(data_dir) if f.startswith('enhanced_crawl_summary') and f.endswith('.json')]
    
    if summary_files:
        latest_summary = sorted(summary_files)[-1]
        summary_path = os.path.join(data_dir, latest_summary)
        
        try:
            with open(summary_path, 'r', encoding='utf-8') as f:
                summary = json.load(f)
            
            print(f"ğŸ“… í¬ë¡¤ë§ ì‹œê°„: {summary.get('crawl_time', 'ì‹œê°„ ì •ë³´ ì—†ìŒ')}")
            print(f"ğŸ”§ í¬ë¡¤ëŸ¬ íƒ€ì…: {summary.get('crawler_type', 'íƒ€ì… ì •ë³´ ì—†ìŒ')}")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘ í•­ëª©: {summary.get('total_items', 0)}ê°œ")
            print(f"ğŸ“ ìš”ì•½: {summary.get('summary', 'ìš”ì•½ ì •ë³´ ì—†ìŒ')}")
            print("ğŸ“ˆ í‚¤ì›Œë“œ ëª©ë¡:")
            
            keywords = summary.get('keywords', [])
            for i, keyword in enumerate(keywords, 1):
                print(f"   {i}. {keyword}")
                
        except Exception as e:
            print(f"âŒ ìš”ì•½ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
    else:
        print("âŒ í¬ë¡¤ë§ ìš”ì•½ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    check_enhanced_data()
    check_summary() 