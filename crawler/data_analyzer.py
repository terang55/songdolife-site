"""
ë…¼í˜„ë™ ì •ë³´ ë°ì´í„° ë¶„ì„ê¸°
ìˆ˜ì§‘ëœ ë‰´ìŠ¤, ì¹´í˜ ê¸€ ë“±ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µ
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from collections import Counter
import re
from loguru import logger
import config

class NonhyeonDataAnalyzer:
    def __init__(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.data_dir = config.DATA_DIR
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logger.add(
            f"{config.LOGS_DIR}/analyzer_{datetime.now().strftime('%Y%m%d')}.log",
            format=config.LOG_FORMAT,
            level=config.LOG_LEVEL,
            rotation="1 day"
        )
        
    def load_recent_data(self, data_type, days=7):
        """ìµœê·¼ Nì¼ê°„ì˜ ë°ì´í„° ë¡œë“œ"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            all_data = []
            
            data_path = f"{self.data_dir}/{data_type}"
            if not os.path.exists(data_path):
                logger.warning(f"ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {data_path}")
                return []
            
            for file in os.listdir(data_path):
                if file.endswith('.json'):
                    file_path = os.path.join(data_path, file)
                    file_time = datetime.fromtimestamp(os.path.getctime(file_path))
                    
                    if file_time >= cutoff_date:
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if isinstance(data, list):
                                    all_data.extend(data)
                                else:
                                    all_data.append(data)
                        except Exception as e:
                            logger.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file} - {str(e)}")
            
            logger.info(f"{data_type} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(all_data)}ê°œ í•­ëª©")
            return all_data
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            return []

    def analyze_news_trends(self, days=7):
        """ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            logger.info(f"ìµœê·¼ {days}ì¼ê°„ ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
            
            news_data = self.load_recent_data("news", days)
            if not news_data:
                return {"error": "ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
            
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(news_data)
            
            # ê¸°ë³¸ í†µê³„
            total_articles = len(df)
            unique_sources = df['press'].nunique() if 'press' in df.columns else 0
            
            # í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìˆ˜
            keyword_counts = df['keyword'].value_counts().to_dict() if 'keyword' in df.columns else {}
            
            # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜
            press_counts = df['press'].value_counts().head(10).to_dict() if 'press' in df.columns else {}
            
            # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            title_keywords = self.extract_keywords_from_titles(df['title'].tolist() if 'title' in df.columns else [])
            
            # ì¼ë³„ ê¸°ì‚¬ ìˆ˜ (ë§Œì•½ ë‚ ì§œ ì •ë³´ê°€ ìˆë‹¤ë©´)
            daily_counts = {}
            if 'crawled_at' in df.columns:
                df['date'] = pd.to_datetime(df['crawled_at']).dt.date
                daily_counts = df['date'].value_counts().sort_index().to_dict()
            
            analysis_result = {
                "period": f"ìµœê·¼ {days}ì¼",
                "total_articles": total_articles,
                "unique_sources": unique_sources,
                "keyword_distribution": keyword_counts,
                "top_press": press_counts,
                "trending_keywords": title_keywords,
                "daily_articles": {str(k): v for k, v in daily_counts.items()},
                "analyzed_at": datetime.now().isoformat()
            }
            
            logger.info(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ: {total_articles}ê°œ ê¸°ì‚¬ ë¶„ì„")
            return analysis_result
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def analyze_cafe_trends(self, days=7):
        """ì¹´í˜ ê¸€ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            logger.info(f"ìµœê·¼ {days}ì¼ê°„ ì¹´í˜ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘")
            
            cafe_data = self.load_recent_data("cafe", days)
            if not cafe_data:
                return {"error": "ë¶„ì„í•  ì¹´í˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
            
            df = pd.DataFrame(cafe_data)
            
            # ê¸°ë³¸ í†µê³„
            total_posts = len(df)
            unique_cafes = df['cafe_name'].nunique() if 'cafe_name' in df.columns else 0
            
            # í‚¤ì›Œë“œë³„ ê¸€ ìˆ˜
            keyword_counts = df['keyword'].value_counts().to_dict() if 'keyword' in df.columns else {}
            
            # ì¹´í˜ë³„ ê¸€ ìˆ˜
            cafe_counts = df['cafe_name'].value_counts().head(10).to_dict() if 'cafe_name' in df.columns else {}
            
            # ì œëª©ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            title_keywords = self.extract_keywords_from_titles(df['title'].tolist() if 'title' in df.columns else [])
            
            # ë‚´ìš©ì—ì„œ ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            content_keywords = self.extract_content_keywords(df['content_preview'].tolist() if 'content_preview' in df.columns else [])
            
            analysis_result = {
                "period": f"ìµœê·¼ {days}ì¼",
                "total_posts": total_posts,
                "unique_cafes": unique_cafes,
                "keyword_distribution": keyword_counts,
                "active_cafes": cafe_counts,
                "trending_keywords": title_keywords,
                "content_interests": content_keywords,
                "analyzed_at": datetime.now().isoformat()
            }
            
            logger.info(f"ì¹´í˜ íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ: {total_posts}ê°œ ê¸€ ë¶„ì„")
            return analysis_result
            
        except Exception as e:
            logger.error(f"ì¹´í˜ íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def extract_keywords_from_titles(self, titles):
        """ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ë…¼í˜„ë™ ê´€ë ¨ í‚¤ì›Œë“œë“¤
            interest_keywords = [
                "ë§›ì§‘", "ì¹´í˜", "ìœ¡ì•„", "ì–´ë¦°ì´ì§‘", "ìœ ì¹˜ì›", "í•™ì›",
                "ë¶€ë™ì‚°", "ì•„íŒŒíŠ¸", "ì „ì„¸", "ë§¤ë§¤", "ë¶„ì–‘",
                "êµí†µ", "ë²„ìŠ¤", "ì§€í•˜ì² ", "ì£¼ì°¨",
                "ë³‘ì›", "ì•½êµ­", "ë§ˆíŠ¸", "í¸ì˜ì ",
                "ê³µì›", "ë†€ì´í„°", "í—¬ìŠ¤ì¥", "ìˆ˜ì˜ì¥",
                "ì´ì‚¬", "ì…ì£¼", "ì‹ ê·œ", "ì˜¤í”ˆ"
            ]
            
            keyword_counts = Counter()
            
            for title in titles:
                if title:
                    for keyword in interest_keywords:
                        if keyword in title:
                            keyword_counts[keyword] += 1
            
            return dict(keyword_counts.most_common(15))
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {}

    def extract_content_keywords(self, contents):
        """ë‚´ìš©ì—ì„œ ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ìœ¡ì•„, ìƒí™œ ê´€ë ¨ í‚¤ì›Œë“œë“¤
            lifestyle_keywords = [
                "ìœ¡ì•„", "ì•„ì´", "ì•„ê¸°", "ì–´ë¨¸ë‹˜", "ì—„ë§ˆ", "ì•„ë¹ ",
                "ë†€ì´", "ìˆ˜ì—…", "ì²´í—˜", "í”„ë¡œê·¸ë¨", "ì´ë²¤íŠ¸",
                "í• ì¸", "ì„¸ì¼", "ì¶”ì²œ", "í›„ê¸°", "ë¦¬ë·°",
                "ë§›ìˆ", "ì¢‹ì•˜", "ê´œì°®", "ë§Œì¡±", "ì¶”ì²œ",
                "ê°€ê²©", "ë¹„ìš©", "ë¬´ë£Œ", "ì €ë ´", "ë¹„ì‹¸"
            ]
            
            keyword_counts = Counter()
            
            for content in contents:
                if content:
                    for keyword in lifestyle_keywords:
                        if keyword in content:
                            keyword_counts[keyword] += 1
            
            return dict(keyword_counts.most_common(10))
            
        except Exception as e:
            logger.error(f"ë‚´ìš© í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {}

    def generate_summary_report(self, days=7):
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            logger.info(f"ìµœê·¼ {days}ì¼ê°„ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            
            # ë‰´ìŠ¤ì™€ ì¹´í˜ ë°ì´í„° ë¶„ì„
            news_analysis = self.analyze_news_trends(days)
            cafe_analysis = self.analyze_cafe_trends(days)
            
            # ì „ì²´ ë°ì´í„° ìš”ì•½
            total_items = 0
            if 'total_articles' in news_analysis:
                total_items += news_analysis['total_articles']
            if 'total_posts' in cafe_analysis:
                total_items += cafe_analysis['total_posts']
            
            # í•« í† í”½ ì¶”ì¶œ (ë‰´ìŠ¤ì™€ ì¹´í˜ì—ì„œ ê³µí†µìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ” í‚¤ì›Œë“œ)
            hot_topics = self.find_common_keywords(news_analysis, cafe_analysis)
            
            report = {
                "summary": {
                    "period": f"ìµœê·¼ {days}ì¼",
                    "total_data_points": total_items,
                    "generated_at": datetime.now().isoformat()
                },
                "news_analysis": news_analysis,
                "cafe_analysis": cafe_analysis,
                "hot_topics": hot_topics,
                "insights": self.generate_insights(news_analysis, cafe_analysis)
            }
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            report_file = f"{self.data_dir}/analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file}")
            return report
            
        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {"error": f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    def find_common_keywords(self, news_analysis, cafe_analysis):
        """ë‰´ìŠ¤ì™€ ì¹´í˜ì—ì„œ ê³µí†µìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°"""
        try:
            news_keywords = set(news_analysis.get('trending_keywords', {}).keys())
            cafe_keywords = set(cafe_analysis.get('trending_keywords', {}).keys())
            
            common_keywords = news_keywords.intersection(cafe_keywords)
            
            hot_topics = {}
            for keyword in common_keywords:
                news_count = news_analysis.get('trending_keywords', {}).get(keyword, 0)
                cafe_count = cafe_analysis.get('trending_keywords', {}).get(keyword, 0)
                hot_topics[keyword] = {
                    "news_mentions": news_count,
                    "cafe_mentions": cafe_count,
                    "total_mentions": news_count + cafe_count
                }
            
            # ì´ ì–¸ê¸‰ìˆ˜ë¡œ ì •ë ¬
            sorted_topics = dict(sorted(hot_topics.items(), 
                                     key=lambda x: x[1]['total_mentions'], 
                                     reverse=True))
            
            return sorted_topics
            
        except Exception as e:
            logger.error(f"ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸° ì˜¤ë¥˜: {str(e)}")
            return {}

    def generate_insights(self, news_analysis, cafe_analysis):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        try:
            # ë‰´ìŠ¤ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
            if 'total_articles' in news_analysis:
                total_news = news_analysis['total_articles']
                if total_news > 0:
                    insights.append(f"ğŸ“° ìµœê·¼ ë…¼í˜„ë™ ê´€ë ¨ ë‰´ìŠ¤ê°€ {total_news}ê±´ ë³´ë„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    top_keyword = list(news_analysis.get('keyword_distribution', {}).keys())
                    if top_keyword:
                        insights.append(f"ğŸ”¥ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œëŠ” '{top_keyword[0]}'ì…ë‹ˆë‹¤.")
            
            # ì¹´í˜ ê´€ë ¨ ì¸ì‚¬ì´íŠ¸
            if 'total_posts' in cafe_analysis:
                total_posts = cafe_analysis['total_posts']
                if total_posts > 0:
                    insights.append(f"ğŸ’¬ ìµœê·¼ ë…¼í˜„ë™ ê´€ë ¨ ì¹´í˜ ê¸€ì´ {total_posts}ê±´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    active_cafes = list(cafe_analysis.get('active_cafes', {}).keys())
                    if active_cafes:
                        insights.append(f"â˜• ê°€ì¥ í™œë°œí•œ ì¹´í˜ëŠ” '{active_cafes[0]}'ì…ë‹ˆë‹¤.")
            
            # ê´€ì‹¬ì‚¬ ë¶„ì„
            content_interests = cafe_analysis.get('content_interests', {})
            if content_interests:
                top_interest = list(content_interests.keys())[0]
                insights.append(f"ğŸ¯ ì£¼ë¯¼ë“¤ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬ëŠ” '{top_interest}' ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤.")
            
            return insights
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return ["ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = NonhyeonDataAnalyzer()
    
    print("ğŸ“Š ë…¼í˜„ë™ ì •ë³´ ë°ì´í„° ë¶„ì„ê¸°")
    print("=" * 50)
    
    while True:
        print("\nğŸ“‹ ë¶„ì„ ë©”ë‰´:")
        print("1. ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ (ìµœê·¼ 7ì¼)")
        print("2. ì¹´í˜ ê¸€ íŠ¸ë Œë“œ ë¶„ì„ (ìµœê·¼ 7ì¼)")
        print("3. ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±")
        print("4. ì‚¬ìš©ì ì •ì˜ ê¸°ê°„ ë¶„ì„")
        print("5. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-5): ").strip()
        
        if choice == "1":
            result = analyzer.analyze_news_trends()
            print("\nğŸ“° ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
            
        elif choice == "2":
            result = analyzer.analyze_cafe_trends()
            print("\nâ˜• ì¹´í˜ ê¸€ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
            
        elif choice == "3":
            result = analyzer.generate_summary_report()
            print("\nğŸ“Š ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸:")
            if 'insights' in result:
                print("\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
                for insight in result['insights']:
                    print(f"   {insight}")
            print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        elif choice == "4":
            try:
                days = int(input("ë¶„ì„í•  ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš” (ì¼ìˆ˜): "))
                result = analyzer.generate_summary_report(days)
                print(f"\nğŸ“Š ìµœê·¼ {days}ì¼ê°„ ë¶„ì„ ì™„ë£Œ")
            except ValueError:
                print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
        elif choice == "5":
            print("ğŸ‘‹ ë¶„ì„ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 