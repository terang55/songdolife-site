#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
- CSS ì…€ë ‰í„° í™•ì¸
- í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
- ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
"""

from enhanced_crawler import EnhancedNonhyeonCrawler
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import time
import urllib.parse

def debug_cafe_crawling():
    """ì¹´í˜ í¬ë¡¤ë§ ë””ë²„ê¹…"""
    print("=== ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ë””ë²„ê¹… ===")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = EnhancedNonhyeonCrawler()
    
    try:
        # ì›¹ë“œë¼ì´ë²„ ìƒì„±
        if not crawler.create_webdriver():
            print("âŒ ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            return
        
        keyword = "ì¸ì²œë…¼í˜„"
        print(f"ğŸ” '{keyword}' ì¹´í˜ ê²€ìƒ‰ ë””ë²„ê¹… ì‹œì‘...")
        
        # ë„¤ì´ë²„ ì¹´í˜ ê²€ìƒ‰ URL ìƒì„±
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"https://search.naver.com/search.naver?ssc=tab.cafe.all&query={encoded_keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=0&ds=&de=&mynews=0&cluster_rank=41&start=1"
        
        print(f"ğŸŒ ê²€ìƒ‰ URL: {search_url}")
        
        # í˜ì´ì§€ ë¡œë“œ
        crawler.driver.get(search_url)
        time.sleep(3)
        
        print(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {crawler.driver.title}")
        
        # í˜„ì¬ í˜ì´ì§€ HTML ì¼ë¶€ ì €ì¥ (ë””ë²„ê¹…ìš©)
        page_source = crawler.driver.page_source
        with open("debug_cafe_page.html", "w", encoding="utf-8") as f:
            f.write(page_source)
        print("ğŸ“ í˜ì´ì§€ HTMLì´ 'debug_cafe_page.html'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê¸°ì¡´ ì…€ë ‰í„°ë¡œ ìš”ì†Œ ì°¾ê¸° ì‹œë„
        print("\n=== ê¸°ì¡´ CSS ì…€ë ‰í„° í…ŒìŠ¤íŠ¸ ===")
        
        selectors_to_test = [
            ".total_wrap .api_subject_bx",
            ".api_subject_bx",
            ".total_wrap",
            ".cafe_item",
            "[data-cr-area='cafe']",
            ".lst_total",
            ".api_ani_send",
            ".total_group"
        ]
        
        for selector in selectors_to_test:
            try:
                elements = crawler.driver.find_elements(By.CSS_SELECTOR, selector)
                print(f"âœ… '{selector}': {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                if len(elements) > 0:
                    print(f"   ì²« ë²ˆì§¸ ìš”ì†Œ í…ìŠ¤íŠ¸: {elements[0].text[:100]}...")
            except Exception as e:
                print(f"âŒ '{selector}': ì˜¤ë¥˜ - {str(e)}")
        
        # ì¹´í˜ ê´€ë ¨ ëª¨ë“  ìš”ì†Œ ì°¾ê¸°
        print("\n=== ì¹´í˜ ê´€ë ¨ ìš”ì†Œ ê²€ìƒ‰ ===")
        
        # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì¹´í˜ ê²Œì‹œê¸€ ìš”ì†Œ ì°¾ê¸°
        possible_selectors = [
            "div[data-cr-area='cafe']",
            ".result_area.cafe",
            ".cafe .total_wrap",
            "[class*='cafe']",
            "[class*='total']",
            ".lst_type",
            ".search_list",
            ".result"
        ]
        
        best_selector = None
        max_elements = 0
        
        for selector in possible_selectors:
            try:
                elements = crawler.driver.find_elements(By.CSS_SELECTOR, selector)
                if len(elements) > max_elements:
                    max_elements = len(elements)
                    best_selector = selector
                print(f"ğŸ” '{selector}': {len(elements)}ê°œ")
            except:
                continue
        
        if best_selector and max_elements > 0:
            print(f"\nğŸ¯ ìµœì  ì…€ë ‰í„°: '{best_selector}' ({max_elements}ê°œ ìš”ì†Œ)")
            
            # ìµœì  ì…€ë ‰í„°ë¡œ ìƒì„¸ ë¶„ì„
            elements = crawler.driver.find_elements(By.CSS_SELECTOR, best_selector)
            
            for i, element in enumerate(elements[:3]):
                print(f"\n--- ìš”ì†Œ {i+1} ë¶„ì„ ---")
                print(f"HTML: {element.get_attribute('outerHTML')[:200]}...")
                print(f"í…ìŠ¤íŠ¸: {element.text[:100]}...")
                
                # ì œëª© ì°¾ê¸° ì‹œë„
                title_selectors = [
                    "a[class*='tit']",
                    ".title a",
                    "a",
                    "[class*='title']",
                    "h3 a",
                    "h4 a"
                ]
                
                for title_sel in title_selectors:
                    try:
                        title_elements = element.find_elements(By.CSS_SELECTOR, title_sel)
                        if title_elements:
                            print(f"  ì œëª© í›„ë³´ ('{title_sel}'): {title_elements[0].text}")
                            break
                    except:
                        continue
        
        # ì‹¤ì œ í¬ë¡¤ë§ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
        print(f"\n=== ì‹¤ì œ í¬ë¡¤ë§ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===")
        cafe_data = crawler.crawl_naver_cafe_search(keyword)
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {len(cafe_data)}ê°œ")
        
        if cafe_data:
            print("\n=== ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ ===")
            for i, post in enumerate(cafe_data):
                print(f"{i+1}. {post.get('title', 'No Title')}")
                print(f"   ì¹´í˜: {post.get('source', 'Unknown')}")
                print(f"   URL: {post.get('url', 'No URL')}")
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # í˜ì´ì§€ì— 'ì¹´í˜' í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
            if "ì¹´í˜" in page_source:
                print("âœ… í˜ì´ì§€ì— 'ì¹´í˜' í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
            else:
                print("âŒ í˜ì´ì§€ì— 'ì¹´í˜' í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        try:
            crawler.driver.save_screenshot("debug_cafe_screenshot.png")
            print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ·ì´ 'debug_cafe_screenshot.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except:
            print("âŒ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨")
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {str(e)}")
    
    finally:
        # ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ
        if crawler.driver:
            print("\nğŸ”š ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ (5ì´ˆ í›„)")
            time.sleep(5)  # ê²°ê³¼ í™•ì¸ ì‹œê°„
            crawler.driver.quit()

if __name__ == "__main__":
    debug_cafe_crawling() 