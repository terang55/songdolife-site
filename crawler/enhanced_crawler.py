"""
ë…¼í˜„ë™ ì •ë³´ í—ˆë¸Œ - ê°œì„ ëœ í¬ë¡¤ëŸ¬
ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ë¡œ ì´ë™í•´ì„œ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ëŸ¬
"""

import os
import json
import time
import requests
import urllib.parse
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from loguru import logger
import config

class EnhancedNonhyeonCrawler:
    def __init__(self):
        """ê°œì„ ëœ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.setup_logging()
        self.driver = None
        self.data_dir = self.ensure_data_directory()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = f"{config.LOGS_DIR}/enhanced_crawler_{datetime.now().strftime('%Y%m%d')}.log"
        os.makedirs(config.LOGS_DIR, exist_ok=True)
        
        logger.add(
            log_file,
            format=config.LOG_FORMAT,
            level=config.LOG_LEVEL,
            rotation="1 day",
            retention="30 days"
        )
        logger.info("ê°œì„ ëœ ë…¼í˜„ë™ í¬ë¡¤ëŸ¬ ì‹œì‘")

    def ensure_data_directory(self):
        """ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±"""
        os.makedirs(config.DATA_DIR, exist_ok=True)
        os.makedirs(f"{config.DATA_DIR}/enhanced_news", exist_ok=True)
        return config.DATA_DIR

    def create_webdriver(self):
        """ì•ˆì „í•œ ì›¹ë“œë¼ì´ë²„ ìƒì„±"""
        try:
            options = webdriver.ChromeOptions()
            for option in config.CHROME_OPTIONS:
                options.add_argument(option)
            
            # ê°œì¸ì •ë³´ ë³´í˜¸ ì„¤ì •
            prefs = {
                "profile.default_content_setting_values": {
                    "notifications": 2,
                    "media_stream": 2,
                }
            }
            options.add_experimental_option("prefs", prefs)
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(30)
            logger.info("ì›¹ë“œë¼ì´ë²„ ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    def extract_article_content(self, url):
        """ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ - í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ"""
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ë°”ë¡œ ìš”ì•½ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ ë©”ì„œë“œëŠ” ë¹„í™œì„±í™”
        return ""

    def crawl_enhanced_naver_news(self, keyword):
        """ê°œì„ ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ - ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ í˜ì´ì§€ì—ì„œ ì§ì ‘ ìˆ˜ì§‘"""
        try:
            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ í˜ì´ì§€ì—ì„œ ì§ì ‘ í¬ë¡¤ë§ ì‹œì‘: {keyword}")
            
            news_data = []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œë§Œ ìˆ˜ì§‘
            search_news = self._crawl_naver_news_search(keyword)
            news_data.extend(search_news)
            
            # ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì •ë¦¬
            unique_news = []
            seen_urls = set()
            
            for article in news_data:
                if article['url'] not in seen_urls:
                    unique_news.append(article)
                    seen_urls.add(article['url'])
            
            news_data = unique_news[:5]  # ìµœëŒ€ 5ê°œ ë‰´ìŠ¤ë§Œ ìœ ì§€
            
            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì§ì ‘ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_data)}ê°œ ê¸°ì‚¬")
            return news_data
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì§ì ‘ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []



    def _crawl_naver_news_search(self, keyword):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í¬ë¡¤ë§ - ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        try:
            logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ '{keyword}' ìˆ˜ì§‘ ì¤‘...")
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ê´€ë ¨ë„ìˆœ, 1ì£¼ì¼) - URL ì¸ì½”ë”© ì¶”ê°€
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://search.naver.com/search.naver?ssc=tab.news.all&query={encoded_keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=1&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3A1w&is_sug_officeid=0&office_category=0&service_area=0"
            
            logger.debug(f"ê²€ìƒ‰ URL: {search_url}")
            self.driver.get(search_url)
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            news_data = []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
            news_containers = self.driver.find_elements(By.CSS_SELECTOR, "div.sds-comps-vertical-layout.sds-comps-full-layout.I6obO60yNcW8I32mDzvQ")
            
            logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ë°œê²¬: {len(news_containers)}ê°œ")
            
            # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
            for idx, container in enumerate(news_containers[:5]):
                try:
                    # ì œëª© ë§í¬ ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ 
                    title_link_element = container.find_element(By.CSS_SELECTOR, "a.rzROnhjF0RNNRoyDaO81.W035WwZVZIWyuG66e5iI")
                    title_span = title_link_element.find_element(By.CSS_SELECTOR, "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1.sds-comps-text-type-headline1")
                    
                    # ì œëª© í…ìŠ¤íŠ¸ ì¶”ì¶œ (mark íƒœê·¸ ì œê±°)
                    title_html = title_span.get_attribute('innerHTML')
                    title = BeautifulSoup(title_html, 'html.parser').get_text(strip=True)
                    
                    # URL ì¶”ì¶œ
                    link = title_link_element.get_attribute("href")
                    
                    if not title or not link:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ì œëª© ë˜ëŠ” ë§í¬ ì—†ìŒ")
                        continue
                    
                    # ìœ íš¨í•œ ë‰´ìŠ¤ URLì¸ì§€ í™•ì¸
                    if not self._is_valid_news_url_enhanced(link):
                        logger.debug(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë‰´ìŠ¤ URL: {link}")
                        continue
                    
                    # ê´‘ê³ ì„± ì½˜í…ì¸  í•„í„°ë§
                    if self._is_ad_content(title):
                        logger.debug(f"ê´‘ê³ ì„± ì½˜í…ì¸  ì œì™¸: {title[:30]}...")
                        continue
                    
                    # ìš”ì•½ë¬¸ ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ 
                    summary = ""
                    try:
                        summary_link_element = container.find_element(By.CSS_SELECTOR, "a.rzROnhjF0RNNRoyDaO81.ti6bfMWvbomDA5J1fNOX")
                        summary_span = summary_link_element.find_element(By.CSS_SELECTOR, "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-3.sds-comps-text-type-body1")
                        
                        # ìš”ì•½ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (mark íƒœê·¸ ì œê±°)
                        summary_html = summary_span.get_attribute('innerHTML')
                        summary = BeautifulSoup(summary_html, 'html.parser').get_text(strip=True)
                        
                        # ìš”ì•½ë¬¸ì´ ë„ˆë¬´ ê¸´ ê²½ìš° ì²« 200ìë¡œ ì œí•œ
                        if len(summary) > 200:
                            summary = summary[:200] + "..."
                            
                    except NoSuchElementException:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ìš”ì•½ë¬¸ ì—†ìŒ")
                        summary = ""
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ - ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
                    date_info = ""
                    try:
                        # í”„ë¡œí•„ ì •ë³´ ì˜ì—­ì—ì„œ ë‚ ì§œ ì •ë³´ ì°¾ê¸° (ë” êµ¬ì²´ì ì¸ ìœ„ì¹˜)
                        date_element = container.find_element(By.CSS_SELECTOR, ".sds-comps-profile-info-subtext span.sds-comps-text.sds-comps-text-type-body2.sds-comps-text-weight-sm")
                        date_text = date_element.text.strip()
                        
                        # "4ì¼ ì „", "1ì‹œê°„ ì „" ë“±ì˜ ìƒëŒ€ ì‹œê°„ì„ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜
                        if "ì¼ ì „" in date_text:
                            days_ago = int(date_text.replace("ì¼ ì „", "").strip())
                            target_date = datetime.now() - timedelta(days=days_ago)
                            date_info = target_date.strftime("%Y-%m-%d")
                        elif "ì‹œê°„ ì „" in date_text:
                            hours_ago = int(date_text.replace("ì‹œê°„ ì „", "").strip())
                            target_date = datetime.now() - timedelta(hours=hours_ago)
                            date_info = target_date.strftime("%Y-%m-%d")
                        elif "ë¶„ ì „" in date_text:
                            # ë¶„ ì „ì€ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì²˜ë¦¬
                            date_info = datetime.now().strftime("%Y-%m-%d")
                        else:
                            # ë‹¤ë¥¸ í˜•ì‹ì˜ ë‚ ì§œëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                            date_info = date_text
                            
                    except NoSuchElementException:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ë‚ ì§œ ì •ë³´ ì—†ìŒ")
                        date_info = datetime.now().strftime("%Y-%m-%d")
                    except Exception as e:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜ - {str(e)}")
                        date_info = datetime.now().strftime("%Y-%m-%d")
                    
                    # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    press = ""
                    try:
                        # ë¨¼ì € HTMLì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                        try:
                            press_element = container.find_element(By.CSS_SELECTOR, "a.rzROnhjF0RNNRoyDaO81.iIKbAB3hQq_YoGhlYc24 span")
                            press = press_element.text.strip()
                        except NoSuchElementException:
                            pass
                        
                        # ì–¸ë¡ ì‚¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
                        if not press:
                            from urllib.parse import urlparse
                            parsed_url = urlparse(link)
                            domain = parsed_url.netloc
                            
                            # ì£¼ìš” ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ë§¤í•‘ - ì¸ì²œ ì§€ì—­ ì–¸ë¡ ì‚¬ í¬í•¨ í™•ì¥
                            press_mapping = {
                                'sedaily.com': 'ì„œìš¸ê²½ì œ',
                                'kyeongin.com': 'ê²½ì¸ì¼ë³´',
                                'asiatime.co.kr': 'ì•„ì‹œì•„íƒ€ì„',
                                'm-i.kr': 'ë§¤ì¼ì¼ë³´',
                                'incheonilbo.com': 'ì¸ì²œì¼ë³´',
                                'incheonnews.com': 'ì¸ì²œë‰´ìŠ¤',
                                'incheonin.com': 'ì¸ì²œì¸',
                                'joongdo.co.kr': 'ì¤‘ë„ì¼ë³´',
                                'newsis.com': 'ë‰´ì‹œìŠ¤',
                                'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
                                'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',
                                'donga.com': 'ë™ì•„ì¼ë³´',
                                'chosun.com': 'ì¡°ì„ ì¼ë³´',
                                'hani.co.kr': 'í•œê²¨ë ˆ',
                                'khan.co.kr': 'ê²½í–¥ì‹ ë¬¸',
                                'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
                                'etnews.com': 'ì „ìì‹ ë¬¸',
                                'news1.kr': 'ë‰´ìŠ¤1',
                                'nocutnews.co.kr': 'ë…¸ì»·ë‰´ìŠ¤',
                                'ohmynews.com': 'ì˜¤ë§ˆì´ë‰´ìŠ¤',
                                'breaknews.com': 'ë¸Œë ˆì´í¬ë‰´ìŠ¤'
                            }
                            
                            for domain_key, press_name in press_mapping.items():
                                if domain_key in domain:
                                    press = press_name
                                    break
                                    
                            if not press:
                                press = domain
                            
                    except Exception:
                        press = ""
                    
                    # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì„±
                    news_data.append({
                        "title": title,
                        "url": link,
                        "content": summary,
                        "summary": summary,
                        "press": press,
                        "date": date_info,  # ì‹¤ì œ ì¶”ì¶œëœ ë‚ ì§œ ì •ë³´ ì‚¬ìš© (YYYY-MM-DD í˜•ì‹)
                        "crawled_at": datetime.now().strftime("%Y-%m-%d"),  # ì‹œê°„ ì œê±°í•˜ê³  ë‚ ì§œë§Œ
                        "content_length": len(summary),
                        "keyword": keyword,
                        "section": "ê²€ìƒ‰ ê²°ê³¼",
                        "type": "news"
                    })
                    
                    logger.debug(f"ë‰´ìŠ¤ {idx+1} ìˆ˜ì§‘ ì„±ê³µ: {title[:30]}... ({press})")
                    
                except Exception as e:
                    logger.debug(f"ë‰´ìŠ¤ ì•„ì´í…œ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ {len(news_data)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_data
            
        except Exception as e:
            logger.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

    def _is_valid_news_url_enhanced(self, url):
        """ê°•í™”ëœ ë‰´ìŠ¤ URL ìœ íš¨ì„± ê²€ì‚¬ - ë” í¬ìš©ì ìœ¼ë¡œ ìˆ˜ì •"""
        if not url:
            return False
        
        # ì œì™¸í•  íŒ¨í„´ë§Œ í™•ì¸ (ê´‘ê³ , í”„ë¡œëª¨ì…˜ ë“±)
        invalid_patterns = [
            "static/channelPromotion",
            "mkt.naver.com",
            "promotion",
            "ad.naver.com",
            "shopping.naver.com",
            "channelPromotion.html",
            "atrb?channel_id"
        ]
        
        # ê¸°ë³¸ì ìœ¼ë¡œ http/httpsë¡œ ì‹œì‘í•˜ëŠ” URLì´ê³  ì œì™¸ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ìœ íš¨
        has_invalid_pattern = any(pattern in url for pattern in invalid_patterns)
        is_web_url = url.startswith(('http://', 'https://'))
        
        return is_web_url and not has_invalid_pattern

    def _is_ad_content(self, title):
        """ê´‘ê³ ì„± ì½˜í…ì¸  íŒë³„"""
        ad_keywords = [
            "ì–¸ë¡ ì‚¬ ì„ ì •",
            "ì–¸ë¡ ì‚¬ê°€ ì„ ì •í•œ",
            "ë„¤ì´ë²„ ë©”ì¸ì—ì„œ", 
            "êµ¬ë…í•˜ì„¸ìš”",
            "í´ë¦½ í¬ë¦¬ì—ì´í„°",
            "í”¼ë“œí˜• ì½˜í…ì¸ ",
            "ì°½ì‘ìë„ ì§€ì›",
            "Ready, Set, Clip",
            "ì£¼ìš”ê¸°ì‚¬ í˜¹ì€ ì‹¬ì¸µê¸°íš"
        ]
        
        return any(keyword in title for keyword in ad_keywords)

    def crawl_naver_blog_search(self, keyword):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ - ê´€ë ¨ë„ìˆœ, ìµœì‹  7ì¼"""
        try:
            logger.info(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ): {keyword}")
            
            blog_data = []
            # ê´€ë ¨ë„ìˆœ, ìµœì‹  1ì£¼ì¼ í•„í„° - ì‚¬ìš©ì ì œê³µ URL êµ¬ì¡° ì‚¬ìš©
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={encoded_keyword}&nso=so%3Ar%2Cp%3A1w"
            
            self.driver.get(search_url)
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìš”ì†Œë“¤ ì°¾ê¸° - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •  
            blog_containers = self.driver.find_elements(By.CSS_SELECTOR, "div.view_wrap")
            
            logger.info(f"ë°œê²¬ëœ ë¸”ë¡œê·¸ ì»¨í…Œì´ë„ˆ ìˆ˜: {len(blog_containers)}")
            
            for idx, container in enumerate(blog_containers[:5]):  # ìµœì‹  5ê°œë§Œ
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                    title_element = container.find_element(By.CSS_SELECTOR, ".detail_box .title_area a.title_link")
                    title = title_element.text.strip()
                    # mark íƒœê·¸ ì œê±°
                    title = BeautifulSoup(title_element.get_attribute('innerHTML'), 'html.parser').get_text(strip=True)
                    link = title_element.get_attribute("href")
                    
                    # ë‚´ìš© ì¶”ì¶œ
                    try:
                        content_element = container.find_element(By.CSS_SELECTOR, ".detail_box .dsc_area a.dsc_link")
                        content_html = content_element.get_attribute('innerHTML')
                        # mark íƒœê·¸ ì œê±°
                        content = BeautifulSoup(content_html, 'html.parser').get_text(strip=True)
                    except:
                        content = ""
                    
                    # ë¸”ë¡œê·¸ëª… ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì—ì„œ ì¶”ì¶œ
                    try:
                        blog_name_element = container.find_element(By.CSS_SELECTOR, ".user_box_inner .user_info a.name")
                        source = blog_name_element.text.strip()
                    except:
                        source = "ë„¤ì´ë²„ ë¸”ë¡œê·¸"
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ - ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
                    date_info = ""
                    try:
                        date_element = container.find_element(By.CSS_SELECTOR, ".user_box_inner .user_info span.sub")
                        date_text = date_element.text.strip()
                        
                        # "Xì¼ ì „" í˜•ì‹ì„ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜
                        if "ì¼ ì „" in date_text:
                            days_ago = int(date_text.replace("ì¼ ì „", "").strip())
                            actual_date = datetime.now() - timedelta(days=days_ago)
                            date_info = actual_date.strftime("%Y-%m-%d")
                        elif "ì‹œê°„ ì „" in date_text:
                            # ì‹œê°„ ì „ì¸ ê²½ìš° ì˜¤ëŠ˜ ë‚ ì§œ
                            date_info = datetime.now().strftime("%Y-%m-%d")
                        else:
                            # ê¸°íƒ€ ê²½ìš° ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì„¤ì •
                            date_info = datetime.now().strftime("%Y-%m-%d")
                    except Exception as date_error:
                        logger.debug(f"ë¸”ë¡œê·¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {str(date_error)}")
                        date_info = datetime.now().strftime("%Y-%m-%d")
                    
                    if not title or len(title) < 5:
                        continue
                    
                    # ê´‘ê³ ì„± ì½˜í…ì¸  ê°•í™” í•„í„°ë§
                    ad_keywords = [
                        "ì–¸ë¡ ì‚¬ ì„ ì •",
                        "í´ë¦½ í¬ë¦¬ì—ì´í„°",
                        "í”¼ë“œí˜• ì½˜í…ì¸ ", 
                        "ì°½ì‘ìë„ ì§€ì›",
                        "ë„¤ì´ë²„ í´ë¦½",
                        "Ready, Set, Clip",
                        "ì‹ ì²­ ê¸°ê°„",
                        "í¬ë¦¬ì—ì´í„°ë¼ë©´"
                    ]
                    
                    if any(ad_word in title for ad_word in ad_keywords):
                        logger.debug(f"ê´‘ê³ ì„± ë¸”ë¡œê·¸ ì œì™¸: {title[:50]}...")
                        continue
                        
                    # URL íŒ¨í„´ í•„í„°ë§ ê°•í™”
                    excluded_urls = [
                        "mkt.naver.com",
                        "news.naver.com/main/static/",
                        "channelPromotion.html",
                        "atrb?channel_id"
                    ]
                    
                    if any(excluded_url in link for excluded_url in excluded_urls):
                        logger.debug(f"ê´‘ê³  URL ì œì™¸: {link}")
                        continue
                        
                    # ì œëª© ê¸¸ì´ ì²´í¬
                    if len(title) < 8:
                        logger.debug(f"ë¸”ë¡œê·¸ ì œëª© ë„ˆë¬´ ì§§ìŒ: {title}")
                        continue
                    
                    blog_post = {
                        "title": title,
                        "content": content,
                        "date": date_info,  # ì‹¤ì œ ë‚ ì§œ ì •ë³´ ì‚¬ìš©
                        "url": link,
                        "source": source,
                        "type": "blog",
                        "keyword": keyword,
                        "search_rank": idx + 1,
                        "crawled_at": datetime.now().strftime("%Y-%m-%d"),  # ì‹œê°„ ì œê±°í•˜ê³  ë‚ ì§œë§Œ
                        "press": source,  # í†µì¼ì„±ì„ ìœ„í•´ press í•„ë“œë„ ì¶”ê°€
                        "summary": content
                    }
                    
                    blog_data.append(blog_post)
                    logger.debug(f"ë¸”ë¡œê·¸ ìˆ˜ì§‘: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ë¸”ë¡œê·¸ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(blog_data)}ê°œ í¬ìŠ¤íŠ¸")
            return blog_data
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def crawl_youtube_search(self, keyword):
        """ìœ íŠœë¸Œ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ - ê´€ë ¨ë„ìˆœ, ìµœì‹  ì—…ë¡œë“œ"""
        try:
            logger.info(f"ìœ íŠœë¸Œ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹ ì—…ë¡œë“œ, ê´€ë ¨ë„ìˆœ): {keyword}")
            
            youtube_data = []
            # ìœ íŠœë¸Œ ê²€ìƒ‰ URL - ê´€ë ¨ë„ìˆœ, ì´ë²ˆ ì£¼ ì—…ë¡œë“œ
            search_url = f"https://www.youtube.com/results?search_query={keyword}&sp=EgQIBBAB"
            
            self.driver.get(search_url)
            time.sleep(3)  # ìœ íŠœë¸ŒëŠ” ë¡œë”©ì´ ì¢€ ë” í•„ìš”
            
            # ë™ì˜ ë²„íŠ¼ í´ë¦­ (ì²˜ìŒ ë°©ë¬¸ì‹œ)
            try:
                accept_button = self.driver.find_element(By.CSS_SELECTOR, "button[aria-label*='ëª¨ë‘ ìˆ˜ë½'], button[aria-label*='Accept all']")
                accept_button.click()
                time.sleep(2)
            except:
                pass
            
            # ìŠ¤í¬ë¡¤ì„ í†µí•´ ë” ë§ì€ ë¹„ë””ì˜¤ ë¡œë“œ
            self.driver.execute_script("window.scrollTo(0, 1000);")
            time.sleep(2)
            
            # ë¹„ë””ì˜¤ ìš”ì†Œë“¤ ì°¾ê¸°
            video_items = self.driver.find_elements(By.CSS_SELECTOR, "div#contents ytd-video-renderer")
            
            logger.info(f"ë°œê²¬ëœ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜: {len(video_items)}")
            
            for idx, item in enumerate(video_items[:8]):  # ìƒìœ„ 8ê°œ
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_element = item.find_element(By.CSS_SELECTOR, "#video-title")
                    title = title_element.text.strip()
                    link = title_element.get_attribute("href")
                    
                    # ì±„ë„ëª… ì¶”ì¶œ
                    try:
                        channel_element = item.find_element(By.CSS_SELECTOR, "#channel-info #text a")
                        channel = channel_element.text.strip()
                    except:
                        channel = ""
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                    try:
                        views_element = item.find_element(By.CSS_SELECTOR, "#metadata-line span:first-child")
                        views = views_element.text.strip()
                    except:
                        views = ""
                    
                    # ì—…ë¡œë“œ ì‹œê°„ ì¶”ì¶œ
                    try:
                        time_element = item.find_element(By.CSS_SELECTOR, "#metadata-line span:last-child")
                        upload_time = time_element.text.strip()
                    except:
                        upload_time = ""
                    
                    # ì¸ë„¤ì¼ URL ì¶”ì¶œ
                    try:
                        thumbnail_element = item.find_element(By.CSS_SELECTOR, "img")
                        thumbnail = thumbnail_element.get_attribute("src")
                    except:
                        thumbnail = ""
                    
                    if not title or len(title) < 3:
                        continue
                    
                    youtube_video = {
                        "title": title,
                        "url": link,
                        "channel": channel,
                        "views": views,
                        "upload_time": upload_time,
                        "thumbnail": thumbnail,
                        "type": "youtube",
                        "keyword": keyword,
                        "search_rank": idx + 1,
                        "date": upload_time  # ì—…ë¡œë“œ ì‹œê°„ì„ ë‚ ì§œë¡œ ì‚¬ìš©
                    }
                    
                    youtube_data.append(youtube_video)
                    logger.debug(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜ì§‘: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(youtube_data)}ê°œ ë¹„ë””ì˜¤")
            return youtube_data
            
        except Exception as e:
            logger.error(f"ìœ íŠœë¸Œ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def save_enhanced_data(self, data, keyword):
        """ê°œì„ ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.data_dir}/enhanced_news/{keyword}_enhanced_news_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ê°œì„ ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} ({len(data)}ê°œ í•­ëª©)")
            return filename
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return None

    def run_enhanced_crawl(self, keywords):
        """ê°œì„ ëœ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            if not self.create_webdriver():
                logger.error("ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
                return False
            
            all_data = []
            total_keywords = len(keywords)
            
            for idx, keyword in enumerate(keywords, 1):
                print(f"\nğŸ” [{idx}/{total_keywords}] í‚¤ì›Œë“œ: '{keyword}' í¬ë¡¤ë§ ì¤‘...")
                print(f"   ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ)...")
                
                # ë‰´ìŠ¤ í¬ë¡¤ë§
                news_data = self.crawl_enhanced_naver_news(keyword)
                
                print(f"   ğŸ“ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ)...")
                # ë¸”ë¡œê·¸ í¬ë¡¤ë§
                blog_data = self.crawl_naver_blog_search(keyword)
                
                print(f"   ğŸ¥ ìœ íŠœë¸Œ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ ì—…ë¡œë“œ, ê´€ë ¨ë„ìˆœ)...")
                # ìœ íŠœë¸Œ í¬ë¡¤ë§
                youtube_data = self.crawl_youtube_search(keyword)
                
                # ë°ì´í„° í•©ì¹˜ê¸°
                combined_data = news_data + blog_data + youtube_data
                
                if combined_data:
                    # í‚¤ì›Œë“œë³„ íŒŒì¼ ì €ì¥
                    self.save_enhanced_data(combined_data, keyword)
                    all_data.extend(combined_data)
                    print(f"   âœ… ì´ {len(combined_data)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ (ë‰´ìŠ¤: {len(news_data)}, ë¸”ë¡œê·¸: {len(blog_data)}, ìœ íŠœë¸Œ: {len(youtube_data)})")
                else:
                    print(f"   âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
                
                # ìš”ì²­ ê°„ ëŒ€ê¸°
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ì „ì²´ ìš”ì•½ ì €ì¥
            if all_data:
                summary_file = f"{self.data_dir}/enhanced_crawl_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "total_items": len(all_data),
                        "keywords": keywords,
                        "crawl_time": datetime.now().isoformat(),
                        "summary": f"{len(all_data)}ê°œ í•­ëª©ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ë¨"
                    }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ì „ì²´ ê°œì„ ëœ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_data)}ê°œ í•­ëª©")
            
            # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì— ë™ê¸°í™”
            self.sync_to_frontend()
            
            return True
            
        except Exception as e:
            logger.error(f"ê°œì„ ëœ í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return False
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")
    
    def sync_to_frontend(self):
        """í¬ë¡¤ë§ ì™„ë£Œ í›„ í”„ë¡ íŠ¸ì—”ë“œë¡œ ë°ì´í„° ë™ê¸°í™”"""
        try:
            print(f"\nğŸ”„ í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„° ë™ê¸°í™” ì‹œì‘...")
            
            # sync_to_frontend ëª¨ë“ˆ ì„í¬íŠ¸
            from sync_to_frontend import sync_data_to_frontend
            
            # ë™ê¸°í™” ì‹¤í–‰
            success = sync_data_to_frontend()
            
            if success:
                print("âœ… í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì™„ë£Œ!")
                logger.info("í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„° ë™ê¸°í™” ì„±ê³µ")
            else:
                print("âŒ í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì‹¤íŒ¨!")
                logger.warning("í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„° ë™ê¸°í™” ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}") 