"""
ë…¼í˜„ë™ ì •ë³´ í—ˆë¸Œ - ê°œì„ ëœ í¬ë¡¤ëŸ¬
ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ë¡œ ì´ë™í•´ì„œ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ëŸ¬
"""

import os
import json
import time
import requests
import urllib.parse
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from loguru import logger
import config
import difflib
import re

class EnhancedNonhyeonCrawler:
    def __init__(self):
        """ê°œì„ ëœ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.setup_logging()
        self.driver = None
        self.data_dir = self.ensure_data_directory()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = f"{config.LOGS_DIR}/enhanced_crawler_{datetime.now().strftime('%Y%m%d')}.log"
        os.makedirs(config.LOGS_DIR, exist_ok=True)
        
        logger.add(
            log_file,
            format=config.LOG_FORMAT,
            level=config.LOG_LEVEL,
            rotation="1 day",
            retention="30 days"
        )
        logger.info("ê°œì„ ëœ ë…¼í˜„ë™ í¬ë¡¤ëŸ¬ ì‹œì‘")

    def ensure_data_directory(self):
        """ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±"""
        os.makedirs(config.DATA_DIR, exist_ok=True)
        os.makedirs(f"{config.DATA_DIR}/enhanced_news", exist_ok=True)
        return config.DATA_DIR

    def create_webdriver(self):
        """ì•ˆì „í•œ ì›¹ë“œë¼ì´ë²„ ìƒì„±"""
        try:
            options = webdriver.ChromeOptions()
            for option in config.CHROME_OPTIONS:
                options.add_argument(option)
            
            # ê°œì¸ì •ë³´ ë³´í˜¸ ì„¤ì •
            prefs = {
                "profile.default_content_setting_values": {
                    "notifications": 2,
                    "media_stream": 2,
                }
            }
            options.add_experimental_option("prefs", prefs)
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(30)
            logger.info("ì›¹ë“œë¼ì´ë²„ ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    def extract_article_content(self, url):
        """ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ - í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ"""
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ë°”ë¡œ ìš”ì•½ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ ë©”ì„œë“œëŠ” ë¹„í™œì„±í™”
        return ""

    def crawl_enhanced_naver_news(self, keyword):
        """ê°œì„ ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ - ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ í˜ì´ì§€ì—ì„œ ì§ì ‘ ìˆ˜ì§‘"""
        try:
            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ í˜ì´ì§€ì—ì„œ ì§ì ‘ í¬ë¡¤ë§ ì‹œì‘: {keyword}")
            
            news_data = []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œë§Œ ìˆ˜ì§‘
            search_news = self._crawl_naver_news_search(keyword)
            news_data.extend(search_news)
            
            # ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì •ë¦¬
            unique_news = []
            seen_urls = set()
            
            for article in news_data:
                if article['url'] not in seen_urls:
                    unique_news.append(article)
                    seen_urls.add(article['url'])
            
            news_data = unique_news[:5]  # ìµœëŒ€ 5ê°œ ë‰´ìŠ¤ë§Œ ìœ ì§€
            
            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì§ì ‘ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_data)}ê°œ ê¸°ì‚¬")
            return news_data
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì§ì ‘ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def _crawl_naver_news_search(self, keyword):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í¬ë¡¤ë§ - ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ """
        try:
            logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ '{keyword}' ìˆ˜ì§‘ ì¤‘...")
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ê´€ë ¨ë„ìˆœ, 1ì£¼ì¼) - URL ì¸ì½”ë”© ì¶”ê°€
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://search.naver.com/search.naver?ssc=tab.news.all&query={encoded_keyword}&sm=tab_opt&sort=0&photo=0&field=0&pd=1&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3A1w&is_sug_officeid=0&office_category=0&service_area=0"
            
            logger.debug(f"ê²€ìƒ‰ URL: {search_url}")
            self.driver.get(search_url)
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            news_data = []
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
            news_containers = self.driver.find_elements(By.CSS_SELECTOR, "div.sds-comps-vertical-layout.sds-comps-full-layout.I6obO60yNcW8I32mDzvQ")
            
            logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ë°œê²¬: {len(news_containers)}ê°œ")
            
            # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
            for idx, container in enumerate(news_containers[:5]):
                try:
                    # ì œëª© ë§í¬ ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ 
                    title_link_element = container.find_element(By.CSS_SELECTOR, "a.rzROnhjF0RNNRoyDaO81.W035WwZVZIWyuG66e5iI")
                    title_span = title_link_element.find_element(By.CSS_SELECTOR, "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1.sds-comps-text-type-headline1")
                    
                    # ì œëª© í…ìŠ¤íŠ¸ ì¶”ì¶œ (mark íƒœê·¸ ì œê±°)
                    title_html = title_span.get_attribute('innerHTML')
                    title = BeautifulSoup(title_html, 'html.parser').get_text(strip=True)
                    
                    # URL ì¶”ì¶œ
                    link = title_link_element.get_attribute("href")
                    
                    if not title or not link:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ì œëª© ë˜ëŠ” ë§í¬ ì—†ìŒ")
                        continue
                    
                    # ìœ íš¨í•œ ë‰´ìŠ¤ URLì¸ì§€ í™•ì¸
                    if not self._is_valid_news_url_enhanced(link):
                        logger.debug(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë‰´ìŠ¤ URL: {link}")
                        continue
                    
                    # ê´‘ê³ ì„± ì½˜í…ì¸  í•„í„°ë§
                    if self._is_ad_content(title):
                        logger.debug(f"ê´‘ê³ ì„± ì½˜í…ì¸  ì œì™¸: {title[:30]}...")
                        continue
                    
                    # ìš”ì•½ë¬¸ ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ê°œì„ 
                    summary = ""
                    try:
                        summary_link_element = container.find_element(By.CSS_SELECTOR, "a.rzROnhjF0RNNRoyDaO81.ti6bfMWvbomDA5J1fNOX")
                        summary_span = summary_link_element.find_element(By.CSS_SELECTOR, "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-3.sds-comps-text-type-body1")
                        
                        # ìš”ì•½ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (mark íƒœê·¸ ì œê±°)
                        summary_html = summary_span.get_attribute('innerHTML')
                        summary = BeautifulSoup(summary_html, 'html.parser').get_text(strip=True)
                        
                        # ìš”ì•½ë¬¸ì´ ë„ˆë¬´ ê¸´ ê²½ìš° ì²« 200ìë¡œ ì œí•œ
                        if len(summary) > 200:
                            summary = summary[:200] + "..."
                            
                    except NoSuchElementException:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ìš”ì•½ë¬¸ ì—†ìŒ")
                        summary = ""
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ - ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
                    date_info = ""
                    try:
                        # í”„ë¡œí•„ ì •ë³´ ì˜ì—­ì—ì„œ ë‚ ì§œ ì •ë³´ ì°¾ê¸° (ë” êµ¬ì²´ì ì¸ ìœ„ì¹˜)
                        date_element = container.find_element(By.CSS_SELECTOR, ".sds-comps-profile-info-subtext span.sds-comps-text.sds-comps-text-type-body2.sds-comps-text-weight-sm")
                        date_text = date_element.text.strip()
                        
                        # "4ì¼ ì „", "1ì‹œê°„ ì „" ë“±ì˜ ìƒëŒ€ ì‹œê°„ì„ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜
                        if "ì¼ ì „" in date_text:
                            days_ago = int(date_text.replace("ì¼ ì „", "").strip())
                            target_date = datetime.now() - timedelta(days=days_ago)
                            date_info = target_date.strftime("%Y-%m-%d")
                        elif "ì‹œê°„ ì „" in date_text:
                            hours_ago = int(date_text.replace("ì‹œê°„ ì „", "").strip())
                            target_date = datetime.now() - timedelta(hours=hours_ago)
                            date_info = target_date.strftime("%Y-%m-%d")
                        elif "ë¶„ ì „" in date_text:
                            # ë¶„ ì „ì€ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì²˜ë¦¬
                            date_info = datetime.now().strftime("%Y-%m-%d")
                        else:
                            # ë‹¤ë¥¸ í˜•ì‹ì˜ ë‚ ì§œëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                            date_info = date_text
                            
                    except NoSuchElementException:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ë‚ ì§œ ì •ë³´ ì—†ìŒ")
                        date_info = datetime.now().strftime("%Y-%m-%d")
                    except Exception as e:
                        logger.debug(f"ë‰´ìŠ¤ {idx+1}: ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜ - {str(e)}")
                        date_info = datetime.now().strftime("%Y-%m-%d")
                    
                    # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    press = ""
                    try:
                        # ë¨¼ì € HTMLì—ì„œ ì–¸ë¡ ì‚¬ ì •ë³´ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                        try:
                            press_element = container.find_element(By.CSS_SELECTOR, "a.rzROnhjF0RNNRoyDaO81.iIKbAB3hQq_YoGhlYc24 span")
                            press = press_element.text.strip()
                        except NoSuchElementException:
                            pass
                        
                        # ì–¸ë¡ ì‚¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
                        if not press:
                            from urllib.parse import urlparse
                            parsed_url = urlparse(link)
                            domain = parsed_url.netloc
                            
                            # ì£¼ìš” ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ë§¤í•‘ - ì¸ì²œ ì§€ì—­ ì–¸ë¡ ì‚¬ í¬í•¨ í™•ì¥
                            press_mapping = {
                                'sedaily.com': 'ì„œìš¸ê²½ì œ',
                                'kyeongin.com': 'ê²½ì¸ì¼ë³´',
                                'asiatime.co.kr': 'ì•„ì‹œì•„íƒ€ì„',
                                'm-i.kr': 'ë§¤ì¼ì¼ë³´',
                                'incheonilbo.com': 'ì¸ì²œì¼ë³´',
                                'incheonnews.com': 'ì¸ì²œë‰´ìŠ¤',
                                'incheonin.com': 'ì¸ì²œì¸',
                                'joongdo.co.kr': 'ì¤‘ë„ì¼ë³´',
                                'newsis.com': 'ë‰´ì‹œìŠ¤',
                                'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
                                'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',
                                'donga.com': 'ë™ì•„ì¼ë³´',
                                'chosun.com': 'ì¡°ì„ ì¼ë³´',
                                'hani.co.kr': 'í•œê²¨ë ˆ',
                                'khan.co.kr': 'ê²½í–¥ì‹ ë¬¸',
                                'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
                                'etnews.com': 'ì „ìì‹ ë¬¸',
                                'news1.kr': 'ë‰´ìŠ¤1',
                                'nocutnews.co.kr': 'ë…¸ì»·ë‰´ìŠ¤',
                                'ohmynews.com': 'ì˜¤ë§ˆì´ë‰´ìŠ¤',
                                'breaknews.com': 'ë¸Œë ˆì´í¬ë‰´ìŠ¤'
                            }
                            
                            for domain_key, press_name in press_mapping.items():
                                if domain_key in domain:
                                    press = press_name
                                    break
                                    
                            if not press:
                                press = domain
                            
                    except Exception:
                        press = ""
                    
                    # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì„±
                    news_data.append({
                        "title": title,
                        "url": link,
                        "content": summary,
                        "summary": summary,
                        "press": press,
                        "date": date_info,  # ì‹¤ì œ ì¶”ì¶œëœ ë‚ ì§œ ì •ë³´ ì‚¬ìš© (YYYY-MM-DD í˜•ì‹)
                        "crawled_at": datetime.now().strftime("%Y-%m-%d"),  # ì‹œê°„ ì œê±°í•˜ê³  ë‚ ì§œë§Œ
                        "content_length": len(summary),
                        "keyword": keyword,
                        "section": "ê²€ìƒ‰ ê²°ê³¼",
                        "type": "news"
                    })
                    
                    logger.debug(f"ë‰´ìŠ¤ {idx+1} ìˆ˜ì§‘ ì„±ê³µ: {title[:30]}... ({press})")
                    
                except Exception as e:
                    logger.debug(f"ë‰´ìŠ¤ ì•„ì´í…œ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ {len(news_data)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_data
            
        except Exception as e:
            logger.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

    def _is_valid_news_url_enhanced(self, url):
        """ê°•í™”ëœ ë‰´ìŠ¤ URL ìœ íš¨ì„± ê²€ì‚¬ - ë” í¬ìš©ì ìœ¼ë¡œ ìˆ˜ì •"""
        if not url:
            return False
        
        # ì œì™¸í•  íŒ¨í„´ë§Œ í™•ì¸ (ê´‘ê³ , í”„ë¡œëª¨ì…˜ ë“±)
        invalid_patterns = [
            "static/channelPromotion",
            "mkt.naver.com",
            "promotion",
            "ad.naver.com",
            "shopping.naver.com",
            "channelPromotion.html",
            "atrb?channel_id"
        ]
        
        # ê¸°ë³¸ì ìœ¼ë¡œ http/httpsë¡œ ì‹œì‘í•˜ëŠ” URLì´ê³  ì œì™¸ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ìœ íš¨
        has_invalid_pattern = any(pattern in url for pattern in invalid_patterns)
        is_web_url = url.startswith(('http://', 'https://'))
        
        return is_web_url and not has_invalid_pattern

    def _is_ad_content(self, title):
        """ê´‘ê³ ì„± ì½˜í…ì¸  íŒë³„"""
        ad_keywords = [
            "ì–¸ë¡ ì‚¬ ì„ ì •",
            "ì–¸ë¡ ì‚¬ê°€ ì„ ì •í•œ",
            "ë„¤ì´ë²„ ë©”ì¸ì—ì„œ", 
            "êµ¬ë…í•˜ì„¸ìš”",
            "í´ë¦½ í¬ë¦¬ì—ì´í„°",
            "í”¼ë“œí˜• ì½˜í…ì¸ ",
            "ì°½ì‘ìë„ ì§€ì›",
            "Ready, Set, Clip",
            "ì£¼ìš”ê¸°ì‚¬ í˜¹ì€ ì‹¬ì¸µê¸°íš"
        ]
        
        return any(keyword in title for keyword in ad_keywords)

    def crawl_naver_blog_search(self, keyword):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ - ê´€ë ¨ë„ìˆœ, ìµœì‹  7ì¼"""
        try:
            logger.info(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ): {keyword}")
            
            blog_data = []
            # ê´€ë ¨ë„ìˆœ, ìµœì‹  1ì£¼ì¼ í•„í„° - ì‚¬ìš©ì ì œê³µ URL êµ¬ì¡° ì‚¬ìš©
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={encoded_keyword}&nso=so%3Ar%2Cp%3A1w"
            
            self.driver.get(search_url)
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìš”ì†Œë“¤ ì°¾ê¸° - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •  
            blog_containers = self.driver.find_elements(By.CSS_SELECTOR, "div.view_wrap")
            
            logger.info(f"ë°œê²¬ëœ ë¸”ë¡œê·¸ ì»¨í…Œì´ë„ˆ ìˆ˜: {len(blog_containers)}")
            
            for idx, container in enumerate(blog_containers[:5]):  # ìµœì‹  5ê°œë§Œ
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                    title_element = container.find_element(By.CSS_SELECTOR, ".detail_box .title_area a.title_link")
                    title = title_element.text.strip()
                    # mark íƒœê·¸ ì œê±°
                    title = BeautifulSoup(title_element.get_attribute('innerHTML'), 'html.parser').get_text(strip=True)
                    link = title_element.get_attribute("href")
                    
                    # ë‚´ìš© ì¶”ì¶œ
                    try:
                        content_element = container.find_element(By.CSS_SELECTOR, ".detail_box .dsc_area a.dsc_link")
                        content_html = content_element.get_attribute('innerHTML')
                        # mark íƒœê·¸ ì œê±°
                        content = BeautifulSoup(content_html, 'html.parser').get_text(strip=True)
                    except:
                        content = ""
                    
                    # ë¸”ë¡œê·¸ëª… ì¶”ì¶œ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì—ì„œ ì¶”ì¶œ
                    try:
                        blog_name_element = container.find_element(By.CSS_SELECTOR, ".user_box_inner .user_info a.name")
                        source = blog_name_element.text.strip()
                    except:
                        source = "ë„¤ì´ë²„ ë¸”ë¡œê·¸"
                    
                    # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ - ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
                    date_info = ""
                    try:
                        date_element = container.find_element(By.CSS_SELECTOR, ".user_box_inner .user_info span.sub")
                        date_text = date_element.text.strip()
                        
                        # "Xì¼ ì „" í˜•ì‹ì„ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜
                        if "ì¼ ì „" in date_text:
                            days_ago = int(date_text.replace("ì¼ ì „", "").strip())
                            actual_date = datetime.now() - timedelta(days=days_ago)
                            date_info = actual_date.strftime("%Y-%m-%d")
                        elif "ì‹œê°„ ì „" in date_text:
                            # ì‹œê°„ ì „ì¸ ê²½ìš° ì˜¤ëŠ˜ ë‚ ì§œ
                            date_info = datetime.now().strftime("%Y-%m-%d")
                        else:
                            # ê¸°íƒ€ ê²½ìš° ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì„¤ì •
                            date_info = datetime.now().strftime("%Y-%m-%d")
                    except Exception as date_error:
                        logger.debug(f"ë¸”ë¡œê·¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {str(date_error)}")
                        date_info = datetime.now().strftime("%Y-%m-%d")
                    
                    if not title or len(title) < 5:
                        continue
                    
                    # ê´‘ê³ ì„± ì½˜í…ì¸  ê°•í™” í•„í„°ë§
                    ad_keywords = [
                        "ì–¸ë¡ ì‚¬ ì„ ì •",
                        "í´ë¦½ í¬ë¦¬ì—ì´í„°",
                        "í”¼ë“œí˜• ì½˜í…ì¸ ", 
                        "ì°½ì‘ìë„ ì§€ì›",
                        "ë„¤ì´ë²„ í´ë¦½",
                        "Ready, Set, Clip",
                        "ì‹ ì²­ ê¸°ê°„",
                        "í¬ë¦¬ì—ì´í„°ë¼ë©´"
                    ]
                    
                    if any(ad_word in title for ad_word in ad_keywords):
                        logger.debug(f"ê´‘ê³ ì„± ë¸”ë¡œê·¸ ì œì™¸: {title[:50]}...")
                        continue
                        
                    # URL íŒ¨í„´ í•„í„°ë§ ê°•í™”
                    excluded_urls = [
                        "mkt.naver.com",
                        "news.naver.com/main/static/",
                        "channelPromotion.html",
                        "atrb?channel_id"
                    ]
                    
                    if any(excluded_url in link for excluded_url in excluded_urls):
                        logger.debug(f"ê´‘ê³  URL ì œì™¸: {link}")
                        continue
                        
                    # ì œëª© ê¸¸ì´ ì²´í¬
                    if len(title) < 8:
                        logger.debug(f"ë¸”ë¡œê·¸ ì œëª© ë„ˆë¬´ ì§§ìŒ: {title}")
                        continue
                    
                    blog_post = {
                        "title": title,
                        "content": content,
                        "date": date_info,  # ì‹¤ì œ ë‚ ì§œ ì •ë³´ ì‚¬ìš©
                        "url": link,
                        "source": source,
                        "type": "blog",
                        "keyword": keyword,
                        "search_rank": idx + 1,
                        "crawled_at": datetime.now().strftime("%Y-%m-%d"),  # ì‹œê°„ ì œê±°í•˜ê³  ë‚ ì§œë§Œ
                        "press": source,  # í†µì¼ì„±ì„ ìœ„í•´ press í•„ë“œë„ ì¶”ê°€
                        "summary": content
                    }
                    
                    blog_data.append(blog_post)
                    logger.debug(f"ë¸”ë¡œê·¸ ìˆ˜ì§‘: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ë¸”ë¡œê·¸ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(blog_data)}ê°œ í¬ìŠ¤íŠ¸")
            return blog_data
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def crawl_youtube_search(self, keyword):
        """ìœ íŠœë¸Œ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ - ê´€ë ¨ë„ìˆœ, ì´ë²ˆì£¼ ì—…ë¡œë“œ"""
        try:
            logger.info(f"ìœ íŠœë¸Œ í¬ë¡¤ë§ ì‹œì‘ (ì´ë²ˆì£¼, ê´€ë ¨ë„ìˆœ): {keyword}")
            
            youtube_data = []
            # ìœ íŠœë¸Œ ê²€ìƒ‰ URL - ê´€ë ¨ë„ìˆœ, ì´ë²ˆì£¼ ì—…ë¡œë“œ (sp=EgIIAw%3D%3D)
            # URL ì¸ì½”ë”©: ì¸ì²œë…¼í˜„ ê²€ìƒ‰, ì´ë²ˆì£¼ í•„í„° ì ìš©
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.youtube.com/results?search_query={encoded_keyword}&sp=EgIIAw%3D%3D"
            
            logger.debug(f"ìœ íŠœë¸Œ ê²€ìƒ‰ URL: {search_url}")
            
            self.driver.get(search_url)
            time.sleep(3)  # ìœ íŠœë¸ŒëŠ” ë¡œë”©ì´ ì¢€ ë” í•„ìš”
            
            # ë™ì˜ ë²„íŠ¼ í´ë¦­ (ì²˜ìŒ ë°©ë¬¸ì‹œ)
            try:
                accept_button = self.driver.find_element(By.CSS_SELECTOR, "button[aria-label*='ëª¨ë‘ ìˆ˜ë½'], button[aria-label*='Accept all']")
                accept_button.click()
                time.sleep(2)
            except:
                pass
            
            # ìŠ¤í¬ë¡¤ì„ í†µí•´ ë” ë§ì€ ë¹„ë””ì˜¤ ë¡œë“œ
            self.driver.execute_script("window.scrollTo(0, 1000);")
            time.sleep(2)
            
            # ë¹„ë””ì˜¤ ìš”ì†Œë“¤ ì°¾ê¸°
            video_items = self.driver.find_elements(By.CSS_SELECTOR, "div#contents ytd-video-renderer")
            
            logger.info(f"ğŸ¬ ë°œê²¬ëœ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜: {len(video_items)}ê°œ (ìƒìœ„ 7ê°œ ìˆ˜ì§‘ ì˜ˆì •)")
            
            for idx, item in enumerate(video_items[:7]):  # ìƒìœ„ 7ê°œë§Œ ìˆ˜ì§‘
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_element = item.find_element(By.CSS_SELECTOR, "#video-title")
                    title = title_element.text.strip()
                    link = title_element.get_attribute("href")
                    
                    # ì±„ë„ëª… ì¶”ì¶œ
                    try:
                        channel_element = item.find_element(By.CSS_SELECTOR, "#channel-info #text a")
                        channel = channel_element.text.strip()
                    except:
                        channel = ""
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                    try:
                        views_element = item.find_element(By.CSS_SELECTOR, "#metadata-line span:first-child")
                        views = views_element.text.strip()
                    except:
                        views = ""
                    
                    # ì—…ë¡œë“œ ì‹œê°„ ì¶”ì¶œ (ì—†ì–´ë„ í¬í•¨)
                    try:
                        time_element = item.find_element(By.CSS_SELECTOR, "#metadata-line span:last-child")
                        upload_time = time_element.text.strip()
                        
                        # ì—…ë¡œë“œ ì‹œê°„ ì²´í¬ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ì œì™¸í•˜ì§€ ì•ŠìŒ
                        if upload_time and not self._is_recent_video(upload_time):
                            logger.debug(f"ì˜¤ë˜ëœ ì˜ìƒì´ì§€ë§Œ í¬í•¨: {title[:30]}... ({upload_time})")
                            
                    except:
                        upload_time = "ì—…ë¡œë“œ ì‹œê°„ ë¶ˆëª…"
                        logger.debug(f"ì—…ë¡œë“œ ì‹œê°„ ë¶ˆëª…ì´ì§€ë§Œ í¬í•¨: {title[:30]}...")
                    
                    # ì¸ë„¤ì¼ URL ì¶”ì¶œ
                    try:
                        thumbnail_element = item.find_element(By.CSS_SELECTOR, "img")
                        thumbnail = thumbnail_element.get_attribute("src")
                    except:
                        thumbnail = ""
                    
                    if not title or len(title) < 3:
                        continue
                    
                    # ì—…ë¡œë“œ ì‹œê°„ì„ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜
                    actual_date = self._convert_upload_time_to_date(upload_time)
                    
                    youtube_video = {
                        "title": title,
                        "url": link,
                        "channel": channel,
                        "views": views,
                        "upload_time": upload_time,
                        "actual_date": actual_date,
                        "thumbnail": thumbnail,
                        "type": "youtube",
                        "keyword": keyword,
                        "search_rank": idx + 1,
                        "date": actual_date,  # ì‹¤ì œ ë‚ ì§œ ì‚¬ìš©
                        "summary": f"ì±„ë„: {channel} | ì¡°íšŒìˆ˜: {views} | ì—…ë¡œë“œ: {upload_time}"
                    }
                    
                    youtube_data.append(youtube_video)
                    logger.debug(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜ì§‘: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(youtube_data)}ê°œ ë¹„ë””ì˜¤")
            return youtube_data
            
        except Exception as e:
            logger.error(f"ìœ íŠœë¸Œ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def _is_recent_video(self, upload_time_text):
        """ìœ íŠœë¸Œ ì—…ë¡œë“œ ì‹œê°„ì´ ìµœê·¼ì¸ì§€ íŒë‹¨ (ê´€ëŒ€í•œ ê¸°ì¤€ ì ìš©)"""
        try:
            if not upload_time_text:
                return True  # ì‹œê°„ ë¶ˆëª…ì¸ ê²½ìš°ë„ í¬í•¨
            
            upload_time_text = upload_time_text.lower().strip()
            
            # ìµœê·¼ ì˜ìƒ íŒ¨í„´ë“¤ (í•œ ë‹¬ ì´ë‚´)
            recent_patterns = [
                "ì´ˆ ì „", "ë¶„ ì „", "ì‹œê°„ ì „",  # ì˜¤ëŠ˜
                "ì¼ ì „", "ì£¼ ì „", "ì£¼ì¼ ì „",  # ì¼/ì£¼ ë‹¨ìœ„
                "í•˜ë£¨ ì „", "ì´í‹€ ì „", "ì‚¬í˜ ì „", "ë‚˜í˜ ì „", "ë‹·ìƒˆ ì „", "ì—¿ìƒˆ ì „",  # í•œê¸€ í‘œí˜„
                "ì¼ì£¼ì¼ ì „", "ì´ì£¼ì¼ ì „", "ì‚¼ì£¼ì¼ ì „", "í•œ ë‹¬ ì „"  # ì£¼/ì›” í‘œí˜„
            ]
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in recent_patterns:
                if pattern in upload_time_text:
                    return True
            
            # "ì¼ ì „" íŒ¨í„´ìœ¼ë¡œ ìˆ«ì ì¶”ì¶œ (30ì¼ê¹Œì§€ í—ˆìš©)
            import re
            day_match = re.search(r'(\d+)ì¼ ì „', upload_time_text)
            if day_match:
                days_ago = int(day_match.group(1))
                return days_ago <= 30  # 30ì¼ ì´ë‚´ë¡œ í™•ì¥
            
            # "ì£¼ ì „" íŒ¨í„´ (4ì£¼ê¹Œì§€ í—ˆìš©)
            week_match = re.search(r'(\d+)ì£¼ ì „', upload_time_text)
            if week_match:
                weeks_ago = int(week_match.group(1))
                return weeks_ago <= 4  # 4ì£¼ ì´ë‚´
            
            # ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
            return True
            
        except Exception as e:
            logger.warning(f"ì—…ë¡œë“œ ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return True  # ì˜¤ë¥˜ ì‹œì—ë„ í¬í•¨

    def _convert_upload_time_to_date(self, upload_time_text):
        """ìœ íŠœë¸Œ ì—…ë¡œë“œ ì‹œê°„ì„ ì‹¤ì œ ë‚ ì§œë¡œ ë³€í™˜"""
        try:
            if not upload_time_text:
                return datetime.now().strftime("%Y-%m-%d")
            
            upload_time_text = upload_time_text.lower().strip()
            now = datetime.now()
            
            # ì˜¤ëŠ˜ ì—…ë¡œë“œ
            if any(x in upload_time_text for x in ["ì´ˆ ì „", "ë¶„ ì „", "ì‹œê°„ ì „"]):
                return now.strftime("%Y-%m-%d")
            
            # Nì¼ ì „ íŒ¨í„´
            import re
            day_match = re.search(r'(\d+)ì¼ ì „', upload_time_text)
            if day_match:
                days_ago = int(day_match.group(1))
                target_date = now - timedelta(days=days_ago)
                return target_date.strftime("%Y-%m-%d")
            
            # í•œê¸€ í‘œí˜„ ì²˜ë¦¬
            korean_days = {
                "í•˜ë£¨ ì „": 1, "ì´í‹€ ì „": 2, "ì‚¬í˜ ì „": 3, 
                "ë‚˜í˜ ì „": 4, "ë‹·ìƒˆ ì „": 5, "ì—¿ìƒˆ ì „": 6
            }
            
            for korean_day, days_ago in korean_days.items():
                if korean_day in upload_time_text:
                    target_date = now - timedelta(days=days_ago)
                    return target_date.strftime("%Y-%m-%d")
            
            # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ ë‚ ì§œ
            return now.strftime("%Y-%m-%d")
            
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜: {str(e)}")
            return datetime.now().strftime("%Y-%m-%d")

    def crawl_naver_cafe_search(self, keyword):
        """ë„¤ì´ë²„ ì¹´í˜ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§"""
        try:
            logger.info(f"ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ì‹œì‘: {keyword}")
            
            cafe_data = []
            
            # ë„¤ì´ë²„ ì¹´í˜ ê²€ìƒ‰ URL (ìµœì‹ ìˆœ)
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://search.naver.com/search.naver?ssc=tab.cafe.all&query={encoded_keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=0&ds=&de=&mynews=0&cluster_rank=41&start=1"
            
            logger.debug(f"ì¹´í˜ ê²€ìƒ‰ URL: {search_url}")
            self.driver.get(search_url)
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ì¹´í˜ ê²Œì‹œê¸€ ìš”ì†Œë“¤ ì°¾ê¸°
            cafe_items = self.driver.find_elements(By.CSS_SELECTOR, ".total_wrap .api_subject_bx")
            
            logger.info(f"â˜• ë°œê²¬ëœ ì¹´í˜ ê²Œì‹œê¸€ ìˆ˜: {len(cafe_items)}ê°œ (ìƒìœ„ 5ê°œ ìˆ˜ì§‘ ì˜ˆì •)")
            
            for idx, item in enumerate(cafe_items[:5]):
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_element = item.find_element(By.CSS_SELECTOR, ".api_txt_lines.total_tit a")
                    title = title_element.text.strip()
                    link = title_element.get_attribute("href")
                    
                    # ì¹´í˜ëª… ì¶”ì¶œ
                    try:
                        cafe_element = item.find_element(By.CSS_SELECTOR, ".sub_txt a")
                        cafe_name = cafe_element.text.strip()
                    except:
                        cafe_name = ""
                    
                    # ì‘ì„±ì ì¶”ì¶œ
                    try:
                        author_element = item.find_element(By.CSS_SELECTOR, ".sub_txt .name")
                        author = author_element.text.strip()
                    except:
                        author = ""
                    
                    # ì‘ì„±ì¼ ì¶”ì¶œ
                    try:
                        date_element = item.find_element(By.CSS_SELECTOR, ".sub_txt .date")
                        date_text = date_element.text.strip()
                        
                        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                        if "." in date_text:
                            # "2024.12.29" í˜•ì‹
                            date_info = date_text.replace(".", "-")
                        elif "ì¼ ì „" in date_text:
                            days_ago = int(date_text.replace("ì¼ ì „", "").strip())
                            target_date = datetime.now() - timedelta(days=days_ago)
                            date_info = target_date.strftime("%Y-%m-%d")
                        else:
                            date_info = datetime.now().strftime("%Y-%m-%d")
                    except:
                        date_info = datetime.now().strftime("%Y-%m-%d")
                    
                    # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ì¶”ì¶œ
                    try:
                        content_element = item.find_element(By.CSS_SELECTOR, ".api_txt_lines.dsc_txt")
                        content = content_element.text.strip()
                    except:
                        content = ""
                    
                    if not title or len(title) < 3:
                        continue
                    
                    # ê´‘ê³ ì„± ì½˜í…ì¸  í•„í„°ë§
                    if self._is_ad_content(title):
                        logger.debug(f"ê´‘ê³ ì„± ì¹´í˜ê¸€ ì œì™¸: {title[:30]}...")
                        continue
                    
                    cafe_post = {
                        "title": title,
                        "content": content,
                        "url": link,
                        "source": cafe_name if cafe_name else "ë„¤ì´ë²„ì¹´í˜",
                        "author": author,
                        "date": date_info,
                        "keyword": keyword,
                        "type": "cafe",
                        "content_length": len(content) if content else 0
                    }
                    
                    cafe_data.append(cafe_post)
                    logger.debug(f"ì¹´í˜ê¸€ ìˆ˜ì§‘: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ì¹´í˜ê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ë„¤ì´ë²„ ì¹´í˜ ìˆ˜ì§‘ ì™„ë£Œ: {len(cafe_data)}ê°œ ê²Œì‹œê¸€")
            return cafe_data
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def _normalize_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ê·œí™” - ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì, ê³µë°±, ìˆ«ì ì œê±°
        text = re.sub(r'[^\wê°€-í£]', '', text)
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', '', text)
        
        return text
    
    def _calculate_similarity(self, text1, text2):
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1 ì‚¬ì´ ê°’)"""
        if not text1 or not text2:
            return 0.0
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™”
        norm_text1 = self._normalize_text(text1)
        norm_text2 = self._normalize_text(text2)
        
        if not norm_text1 or not norm_text2:
            return 0.0
        
        # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = difflib.SequenceMatcher(None, norm_text1, norm_text2).ratio()
        return similarity
    
    def _is_duplicate_content(self, item1, item2, title_threshold=0.8, content_threshold=0.7):
        """ë‘ í•­ëª©ì´ ì¤‘ë³µì¸ì§€ íŒë‹¨"""
        try:
            # URLì´ ê°™ìœ¼ë©´ í™•ì‹¤í•œ ì¤‘ë³µ
            if item1.get('url') == item2.get('url'):
                return True
            
            # ì œëª© ìœ ì‚¬ë„ ê²€ì‚¬
            title1 = item1.get('title', '')
            title2 = item2.get('title', '')
            title_similarity = self._calculate_similarity(title1, title2)
            
            # ë‚´ìš© ìœ ì‚¬ë„ ê²€ì‚¬ (summary ë˜ëŠ” content ì‚¬ìš©)
            content1 = item1.get('summary', '') or item1.get('content', '')
            content2 = item2.get('summary', '') or item2.get('content', '')
            content_similarity = self._calculate_similarity(content1, content2)
            
            # ì œëª©ì´ ë§¤ìš° ìœ ì‚¬í•˜ê±°ë‚˜, ì œëª©ê³¼ ë‚´ìš©ì´ ëª¨ë‘ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
            if title_similarity >= title_threshold:
                return True
            
            if title_similarity >= 0.6 and content_similarity >= content_threshold:
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"ì¤‘ë³µ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def remove_duplicates(self, data_list):
        """ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ í•­ëª© ì œê±°"""
        try:
            if not data_list:
                return []
            
            unique_items = []
            removed_count = 0
            
            for current_item in data_list:
                is_duplicate = False
                
                # ê¸°ì¡´ unique_itemsì™€ ë¹„êµ
                for existing_item in unique_items:
                    if self._is_duplicate_content(current_item, existing_item):
                        is_duplicate = True
                        removed_count += 1
                        logger.debug(f"ì¤‘ë³µ ì œê±°: '{current_item.get('title', '')[:50]}...'")
                        break
                
                if not is_duplicate:
                    unique_items.append(current_item)
            
            logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: ì „ì²´ {len(data_list)}ê°œ â†’ ìœ ë‹ˆí¬ {len(unique_items)}ê°œ (ì œê±°: {removed_count}ê°œ)")
            return unique_items
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return data_list  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜

    def save_enhanced_data(self, data, keyword):
        """ê°œì„ ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.data_dir}/enhanced_news/{keyword}_enhanced_news_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ê°œì„ ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} ({len(data)}ê°œ í•­ëª©)")
            return filename
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return None

    def run_enhanced_crawl_with_platform_keywords(self):
        """í”Œë«í¼ë³„ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•œ ê°œì„ ëœ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            if not self.create_webdriver():
                logger.error("ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
                return False
            
            all_data = []
            platform_results = {}
            
            # í”Œë«í¼ë³„ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
            news_keywords = config.SEARCH_KEYWORDS.get('news', [])
            blog_keywords = config.SEARCH_KEYWORDS.get('blog', [])
            youtube_keywords = config.SEARCH_KEYWORDS.get('youtube', [])
            cafe_keywords = config.SEARCH_KEYWORDS.get('cafe', [])
            
            print(f"ğŸ¯ í”Œë«í¼ë³„ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì‹œì‘...")
            print(f"   ğŸ“° ë‰´ìŠ¤ í‚¤ì›Œë“œ: {len(news_keywords)}ê°œ")
            print(f"   ğŸ“ ë¸”ë¡œê·¸ í‚¤ì›Œë“œ: {len(blog_keywords)}ê°œ") 
            print(f"   ğŸ¥ ìœ íŠœë¸Œ í‚¤ì›Œë“œ: {len(youtube_keywords)}ê°œ")
            print(f"   â˜• ë„¤ì´ë²„ì¹´í˜ í‚¤ì›Œë“œ: {len(cafe_keywords)}ê°œ")
            
            # 1. ë‰´ìŠ¤ í¬ë¡¤ë§
            print(f"\nğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
            news_data = []
            for idx, keyword in enumerate(news_keywords, 1):
                print(f"   [{idx}/{len(news_keywords)}] ë‰´ìŠ¤ í‚¤ì›Œë“œ: '{keyword}'")
                keyword_news = self.crawl_enhanced_naver_news(keyword)
                news_data.extend(keyword_news)
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            print(f"   âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_data)}ê°œ í•­ëª©")
            platform_results['news'] = news_data
            
            # 2. ë¸”ë¡œê·¸ í¬ë¡¤ë§  
            print(f"\nğŸ“ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘...")
            blog_data = []
            for idx, keyword in enumerate(blog_keywords, 1):
                print(f"   [{idx}/{len(blog_keywords)}] ë¸”ë¡œê·¸ í‚¤ì›Œë“œ: '{keyword}'")
                keyword_blog = self.crawl_naver_blog_search(keyword)
                blog_data.extend(keyword_blog)
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            print(f"   âœ… ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(blog_data)}ê°œ í•­ëª©")
            platform_results['blog'] = blog_data
            
            # 3. ìœ íŠœë¸Œ í¬ë¡¤ë§
            print(f"\nğŸ¥ ìœ íŠœë¸Œ í¬ë¡¤ë§ ì‹œì‘...")
            youtube_data = []
            for idx, keyword in enumerate(youtube_keywords, 1):
                print(f"   [{idx}/{len(youtube_keywords)}] ìœ íŠœë¸Œ í‚¤ì›Œë“œ: '{keyword}'")
                keyword_youtube = self.crawl_youtube_search(keyword)
                youtube_data.extend(keyword_youtube)
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            print(f"   âœ… ìœ íŠœë¸Œ ìˆ˜ì§‘ ì™„ë£Œ: {len(youtube_data)}ê°œ í•­ëª©")
            platform_results['youtube'] = youtube_data
            
            # 4. ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§
            print(f"\nâ˜• ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ ì‹œì‘...")
            cafe_data = []
            for idx, keyword in enumerate(cafe_keywords, 1):
                print(f"   [{idx}/{len(cafe_keywords)}] ì¹´í˜ í‚¤ì›Œë“œ: '{keyword}'")
                keyword_cafe = self.crawl_naver_cafe_search(keyword)
                cafe_data.extend(keyword_cafe)
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            print(f"   âœ… ë„¤ì´ë²„ ì¹´í˜ ìˆ˜ì§‘ ì™„ë£Œ: {len(cafe_data)}ê°œ í•­ëª©")
            platform_results['cafe'] = cafe_data
            
            # í”Œë«í¼ë³„ ì¤‘ë³µ ì œê±°
            print(f"\nğŸ” í”Œë«í¼ë³„ ì¤‘ë³µ ì œê±° ì¤‘...")
            for platform, data in platform_results.items():
                if data:
                    unique_data = self.remove_duplicates(data)
                    removed_count = len(data) - len(unique_data)
                    platform_results[platform] = unique_data
                    print(f"   {platform}: {len(data)}ê°œ â†’ {len(unique_data)}ê°œ (ì¤‘ë³µ {removed_count}ê°œ ì œê±°)")
            
            # ì „ì²´ ë°ì´í„° í•©ì¹˜ê¸°
            all_data = []
            for platform_data in platform_results.values():
                all_data.extend(platform_data)
            
            # ì „ì²´ ë°ì´í„°ì—ì„œ ìµœì¢… ì¤‘ë³µ ì œê±° (í”Œë«í¼ ê°„ ì¤‘ë³µ)
            if all_data:
                print(f"\nğŸ” í”Œë«í¼ ê°„ ìµœì¢… ì¤‘ë³µ ê²€ì‚¬ ì¤‘...")
                initial_count = len(all_data)
                final_unique_data = self.remove_duplicates(all_data)
                final_removed_count = initial_count - len(final_unique_data)
                
                if final_removed_count > 0:
                    print(f"âœ… í”Œë«í¼ ê°„ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {final_removed_count}ê°œ ì¶”ê°€ ì¤‘ë³µ ì œê±°")
                
                # í”Œë«í¼ë³„ë¡œ í‚¤ì›Œë“œ ê·¸ë£¹í•‘í•´ì„œ ì €ì¥
                self._save_platform_based_data(platform_results, final_unique_data)
                
                # ì „ì²´ ìš”ì•½ ì €ì¥
                summary_file = f"{self.data_dir}/enhanced_crawl_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "total_items": len(final_unique_data),
                        "total_before_dedup": initial_count,
                        "duplicates_removed": final_removed_count,
                        "platform_breakdown": {
                            "news": {
                                "keywords": news_keywords,
                                "items": len(platform_results['news'])
                            },
                            "blog": {
                                "keywords": blog_keywords, 
                                "items": len(platform_results['blog'])
                            },
                            "youtube": {
                                "keywords": youtube_keywords,
                                "items": len(platform_results['youtube'])
                            },
                            "cafe": {
                                "keywords": cafe_keywords,
                                "items": len(platform_results['cafe'])
                            }
                        },
                        "crawl_time": datetime.now().isoformat(),
                        "summary": f"{len(final_unique_data)}ê°œ ìœ ë‹ˆí¬ í•­ëª©ì´ í”Œë«í¼ë³„ í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ë¨"
                    }, f, ensure_ascii=False, indent=2)
                
                all_data = final_unique_data
            
            logger.info(f"í”Œë«í¼ë³„ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_data)}ê°œ ìœ ë‹ˆí¬ í•­ëª©")
            print(f"\nğŸ‰ í”Œë«í¼ë³„ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"   ğŸ“Š ìµœì¢… ê²°ê³¼: {len(all_data)}ê°œ ìœ ë‹ˆí¬ í•­ëª©")
            print(f"   ğŸ“° ë‰´ìŠ¤: {len(platform_results['news'])}ê°œ")
            print(f"   ğŸ“ ë¸”ë¡œê·¸: {len(platform_results['blog'])}ê°œ")
            print(f"   ğŸ¥ ìœ íŠœë¸Œ: {len(platform_results['youtube'])}ê°œ")
            
            # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì— ë™ê¸°í™”
            self.sync_to_frontend()
            
            return True
            
        except Exception as e:
            logger.error(f"í”Œë«í¼ë³„ í‚¤ì›Œë“œ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return False
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")
    
    def _save_platform_based_data(self, platform_results, final_data):
        """í”Œë«í¼ë³„ ë°ì´í„°ë¥¼ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # í”Œë«í¼ë³„ë¡œ ì €ì¥
            for platform, data in platform_results.items():
                if data:
                    filename = f"{self.data_dir}/enhanced_news/{platform}_enhanced_news_{timestamp}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    logger.info(f"{platform} ë°ì´í„° ì €ì¥: {filename} ({len(data)}ê°œ í•­ëª©)")
            
            # ì „ì²´ í†µí•© ë°ì´í„°ë„ ì €ì¥
            all_filename = f"{self.data_dir}/enhanced_news/all_platforms_enhanced_news_{timestamp}.json"
            with open(all_filename, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            logger.info(f"í†µí•© ë°ì´í„° ì €ì¥: {all_filename} ({len(final_data)}ê°œ í•­ëª©)")
            
        except Exception as e:
            logger.error(f"í”Œë«í¼ë³„ ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {str(e)}")

    def run_enhanced_crawl(self, keywords):
        """ê°œì„ ëœ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            if not self.create_webdriver():
                logger.error("ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
                return False
            
            all_data = []
            total_keywords = len(keywords)
            
            for idx, keyword in enumerate(keywords, 1):
                print(f"\nğŸ” [{idx}/{total_keywords}] í‚¤ì›Œë“œ: '{keyword}' í¬ë¡¤ë§ ì¤‘...")
                print(f"   ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ)...")
                
                # ë‰´ìŠ¤ í¬ë¡¤ë§
                news_data = self.crawl_enhanced_naver_news(keyword)
                
                print(f"   ğŸ“ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ)...")
                # ë¸”ë¡œê·¸ í¬ë¡¤ë§
                blog_data = self.crawl_naver_blog_search(keyword)
                
                print(f"   ğŸ¥ ìœ íŠœë¸Œ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ ì—…ë¡œë“œ, ê´€ë ¨ë„ìˆœ)...")
                # ìœ íŠœë¸Œ í¬ë¡¤ë§
                youtube_data = self.crawl_youtube_search(keyword)
                
                # ë°ì´í„° í•©ì¹˜ê¸°
                combined_data = news_data + blog_data + youtube_data
                
                if combined_data:
                    # í‚¤ì›Œë“œë³„ ì¤‘ë³µ ì œê±°
                    print(f"   ğŸ” ì¤‘ë³µ ê²€ì‚¬ ì¤‘...")
                    unique_data = self.remove_duplicates(combined_data)
                    
                    # í‚¤ì›Œë“œë³„ íŒŒì¼ ì €ì¥
                    self.save_enhanced_data(unique_data, keyword)
                    all_data.extend(unique_data)
                    
                    removed_count = len(combined_data) - len(unique_data)
                    if removed_count > 0:
                        print(f"   âœ… ì´ {len(unique_data)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ (ë‰´ìŠ¤: {len(news_data)}, ë¸”ë¡œê·¸: {len(blog_data)}, ìœ íŠœë¸Œ: {len(youtube_data)}) - ì¤‘ë³µ {removed_count}ê°œ ì œê±°")
                    else:
                        print(f"   âœ… ì´ {len(unique_data)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ (ë‰´ìŠ¤: {len(news_data)}, ë¸”ë¡œê·¸: {len(blog_data)}, ìœ íŠœë¸Œ: {len(youtube_data)})")
                else:
                    print(f"   âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
                
                # ìš”ì²­ ê°„ ëŒ€ê¸°
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ì „ì²´ ë°ì´í„°ì—ì„œ ìµœì¢… ì¤‘ë³µ ì œê±°
            if all_data:
                print(f"\nğŸ” ì „ì²´ ë°ì´í„° ìµœì¢… ì¤‘ë³µ ê²€ì‚¬ ì¤‘...")
                final_unique_data = self.remove_duplicates(all_data)
                final_removed_count = len(all_data) - len(final_unique_data)
                
                if final_removed_count > 0:
                    print(f"âœ… ì „ì²´ ë°ì´í„° ì¤‘ë³µ ì œê±° ì™„ë£Œ: {final_removed_count}ê°œ ì¶”ê°€ ì¤‘ë³µ ì œê±°")
                
                # ì „ì²´ ìš”ì•½ ì €ì¥
                summary_file = f"{self.data_dir}/enhanced_crawl_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "total_items": len(final_unique_data),
                        "total_before_dedup": len(all_data),
                        "duplicates_removed": final_removed_count,
                        "keywords": keywords,
                        "crawl_time": datetime.now().isoformat(),
                        "summary": f"{len(final_unique_data)}ê°œ ìœ ë‹ˆí¬ í•­ëª©ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ë¨ (ì¤‘ë³µ {final_removed_count}ê°œ ì œê±°)"
                    }, f, ensure_ascii=False, indent=2)
                
                # all_dataë¥¼ ìµœì¢… ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
                all_data = final_unique_data
            
            logger.info(f"ì „ì²´ ê°œì„ ëœ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_data)}ê°œ ìœ ë‹ˆí¬ í•­ëª©")
            
            # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í”„ë¡ íŠ¸ì—”ë“œì— ë™ê¸°í™”
            self.sync_to_frontend()
            
            return True
            
        except Exception as e:
            logger.error(f"ê°œì„ ëœ í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return False
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")

    def sync_to_frontend(self):
        """í¬ë¡¤ë§ ì™„ë£Œ í›„ í”„ë¡ íŠ¸ì—”ë“œë¡œ ë°ì´í„° ë™ê¸°í™”"""
        try:
            print(f"\nğŸ”„ í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„° ë™ê¸°í™” ì‹œì‘...")
            
            # sync_to_frontend ëª¨ë“ˆ ì„í¬íŠ¸
            from sync_to_frontend import sync_data_to_frontend
            
            # ë™ê¸°í™” ì‹¤í–‰
            success = sync_data_to_frontend()
            
            if success:
                print("âœ… í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì™„ë£Œ!")
                logger.info("í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„° ë™ê¸°í™” ì„±ê³µ")
            else:
                print("âŒ í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì‹¤íŒ¨!")
                logger.warning("í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„° ë™ê¸°í™” ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}") 