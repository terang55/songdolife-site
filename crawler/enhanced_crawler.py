"""
ë…¼í˜„ë™ ì •ë³´ í—ˆë¸Œ - ê°œì„ ëœ í¬ë¡¤ëŸ¬
ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ë¡œ ì´ë™í•´ì„œ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ëŸ¬
"""

import os
import json
import time
import requests
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from loguru import logger
import config

class EnhancedNonhyeonCrawler:
    def __init__(self):
        """ê°œì„ ëœ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.setup_logging()
        self.driver = None
        self.data_dir = self.ensure_data_directory()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = f"{config.LOGS_DIR}/enhanced_crawler_{datetime.now().strftime('%Y%m%d')}.log"
        os.makedirs(config.LOGS_DIR, exist_ok=True)
        
        logger.add(
            log_file,
            format=config.LOG_FORMAT,
            level=config.LOG_LEVEL,
            rotation="1 day",
            retention="30 days"
        )
        logger.info("ê°œì„ ëœ ë…¼í˜„ë™ í¬ë¡¤ëŸ¬ ì‹œì‘")

    def ensure_data_directory(self):
        """ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±"""
        os.makedirs(config.DATA_DIR, exist_ok=True)
        os.makedirs(f"{config.DATA_DIR}/enhanced_news", exist_ok=True)
        return config.DATA_DIR

    def create_webdriver(self):
        """ì•ˆì „í•œ ì›¹ë“œë¼ì´ë²„ ìƒì„±"""
        try:
            options = webdriver.ChromeOptions()
            for option in config.CHROME_OPTIONS:
                options.add_argument(option)
            
            # ê°œì¸ì •ë³´ ë³´í˜¸ ì„¤ì •
            prefs = {
                "profile.default_content_setting_values": {
                    "notifications": 2,
                    "media_stream": 2,
                }
            }
            options.add_experimental_option("prefs", prefs)
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(30)
            logger.info("ì›¹ë“œë¼ì´ë²„ ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    def extract_article_content(self, url):
        """ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            self.driver.get(url)
            time.sleep(2)
            
            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ëŒ€ì‘)
            content_selectors = [
                "#newsct_article",  # ë„¤ì´ë²„ ë‰´ìŠ¤
                ".news_article",
                ".article_body",
                ".news_content",
                ".content",
                "#content",
                "article",
                ".post_content"
            ]
            
            content = ""
            for selector in content_selectors:
                try:
                    content_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    content = content_element.text.strip()
                    if content and len(content) > 100:
                        break
                except:
                    continue
            
            return content
            
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            return ""

    def crawl_enhanced_naver_news(self, keyword):
        """ê°œì„ ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ - ê´€ë ¨ë„ìˆœ, ìµœì‹  7ì¼, ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ê¹Œì§€ ìˆ˜ì§‘"""
        try:
            logger.info(f"ê°œì„ ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ): {keyword}")
            
            news_data = []
            # ê´€ë ¨ë„ìˆœ(0), ìµœì‹ ìˆœ(1) - ê´€ë ¨ë„ìˆœìœ¼ë¡œ ë³€ê²½, ìµœì‹  7ì¼ í•„í„° ì¶”ê°€
            search_url = f"{config.NAVER_NEWS_BASE_URL}?where=news&query={keyword}&sort=0&pd=3&ds=&de="
            
            self.driver.get(search_url)
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì†Œë“¤ ì°¾ê¸°
            news_items = self.driver.find_elements(By.CSS_SELECTOR, ".list_news .bx")
            
            if not news_items:
                news_items = self.driver.find_elements(By.CSS_SELECTOR, ".group_news .bx")
            
            logger.info(f"ë°œê²¬ëœ ë‰´ìŠ¤ ì•„ì´í…œ ìˆ˜: {len(news_items)}")
            
            for idx, item in enumerate(news_items[:10]):  # ìƒìœ„ 10ê°œë§Œ ìƒì„¸ ë¶„ì„
                try:
                    print(f"   ğŸ“° ê¸°ì‚¬ {idx+1}/10 ì²˜ë¦¬ ì¤‘...")
                    
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_element = item.find_element(By.CSS_SELECTOR, "a.news_tit")
                    title = title_element.text.strip()
                    link = title_element.get_attribute("href")
                    
                    # ìš”ì•½ë¬¸ ì¶”ì¶œ
                    try:
                        summary_element = item.find_element(By.CSS_SELECTOR, ".news_dsc")
                        summary = summary_element.text.strip()
                    except:
                        summary = ""
                    
                    # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                    try:
                        press_element = item.find_element(By.CSS_SELECTOR, ".info_group .press")
                        press = press_element.text.strip()
                    except:
                        press = ""
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    try:
                        date_element = item.find_element(By.CSS_SELECTOR, ".info_group .info")
                        date_text = date_element.text.strip()
                    except:
                        date_text = ""
                    
                    # ê¸°ë³¸ ë°ì´í„° ê²€ì¦
                    if not title or len(title) < 5:
                        continue
                    
                    # ì œì™¸ í‚¤ì›Œë“œ í•„í„°ë§
                    if any(exclude_word in title for exclude_word in config.EXCLUDE_KEYWORDS):
                        continue
                    
                    # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
                    content = self.extract_article_content(link) if link else ""
                    
                    news_article = {
                        "title": title,
                        "url": link,
                        "content": content,
                        "summary": summary,
                        "press": press,
                        "date": date_text,
                        "crawled_at": datetime.now().isoformat(),
                        "content_length": len(content),
                        "keyword": keyword,
                        "search_rank": idx + 1,
                        "type": "news"
                    }
                    
                    news_data.append(news_article)
                    logger.debug(f"ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ê°œì„ ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_data)}ê°œ ê¸°ì‚¬")
            return news_data
            
        except Exception as e:
            logger.error(f"ê°œì„ ëœ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def crawl_naver_blog_search(self, keyword):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ - ê´€ë ¨ë„ìˆœ, ìµœì‹  7ì¼"""
        try:
            logger.info(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ): {keyword}")
            
            blog_data = []
            # ê´€ë ¨ë„ìˆœ(0), ìµœì‹ ìˆœ(1) - ê´€ë ¨ë„ìˆœìœ¼ë¡œ ë³€ê²½, ìµœì‹  7ì¼ í•„í„° ì¶”ê°€
            search_url = f"https://search.naver.com/search.naver?where=post&query={keyword}&sort=0&pd=3&ds=&de="
            
            self.driver.get(search_url)
            time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìš”ì†Œë“¤ ì°¾ê¸°
            blog_items = self.driver.find_elements(By.CSS_SELECTOR, ".list_blog .bx")
            
            logger.info(f"ë°œê²¬ëœ ë¸”ë¡œê·¸ ì•„ì´í…œ ìˆ˜: {len(blog_items)}")
            
            for idx, item in enumerate(blog_items[:5]):  # ìƒìœ„ 5ê°œë§Œ
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_element = item.find_element(By.CSS_SELECTOR, "a.title_link")
                    title = title_element.text.strip()
                    link = title_element.get_attribute("href")
                    
                    # ë‚´ìš© ì¶”ì¶œ
                    try:
                        content_element = item.find_element(By.CSS_SELECTOR, ".dsc_link")
                        content = content_element.text.strip()
                    except:
                        content = ""
                    
                    # ë¸”ë¡œê·¸ëª… ì¶”ì¶œ
                    try:
                        source_element = item.find_element(By.CSS_SELECTOR, ".name")
                        source = source_element.text.strip()
                    except:
                        source = "ë„¤ì´ë²„ ë¸”ë¡œê·¸"
                    
                    if not title or len(title) < 5:
                        continue
                    
                    blog_post = {
                        "title": title,
                        "content": content,
                        "date": "",  # ë¸”ë¡œê·¸ëŠ” ì •í™•í•œ ë‚ ì§œ ì¶”ì¶œì´ ì–´ë ¤ì›€
                        "url": link,
                        "source": source,
                        "type": "blog",
                        "keyword": keyword,
                        "search_rank": idx + 1
                    }
                    
                    blog_data.append(blog_post)
                    logger.debug(f"ë¸”ë¡œê·¸ ìˆ˜ì§‘: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ë¸”ë¡œê·¸ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(blog_data)}ê°œ í¬ìŠ¤íŠ¸")
            return blog_data
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def crawl_youtube_search(self, keyword):
        """ìœ íŠœë¸Œ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§ - ê´€ë ¨ë„ìˆœ, ìµœì‹  ì—…ë¡œë“œ"""
        try:
            logger.info(f"ìœ íŠœë¸Œ í¬ë¡¤ë§ ì‹œì‘ (ìµœì‹ ì—…ë¡œë“œ, ê´€ë ¨ë„ìˆœ): {keyword}")
            
            youtube_data = []
            # ìœ íŠœë¸Œ ê²€ìƒ‰ URL - ê´€ë ¨ë„ìˆœ, ì´ë²ˆ ì£¼ ì—…ë¡œë“œ
            search_url = f"https://www.youtube.com/results?search_query={keyword}&sp=EgQIBBAB"
            
            self.driver.get(search_url)
            time.sleep(3)  # ìœ íŠœë¸ŒëŠ” ë¡œë”©ì´ ì¢€ ë” í•„ìš”
            
            # ë™ì˜ ë²„íŠ¼ í´ë¦­ (ì²˜ìŒ ë°©ë¬¸ì‹œ)
            try:
                accept_button = self.driver.find_element(By.CSS_SELECTOR, "button[aria-label*='ëª¨ë‘ ìˆ˜ë½'], button[aria-label*='Accept all']")
                accept_button.click()
                time.sleep(2)
            except:
                pass
            
            # ìŠ¤í¬ë¡¤ì„ í†µí•´ ë” ë§ì€ ë¹„ë””ì˜¤ ë¡œë“œ
            self.driver.execute_script("window.scrollTo(0, 1000);")
            time.sleep(2)
            
            # ë¹„ë””ì˜¤ ìš”ì†Œë“¤ ì°¾ê¸°
            video_items = self.driver.find_elements(By.CSS_SELECTOR, "div#contents ytd-video-renderer")
            
            logger.info(f"ë°œê²¬ëœ ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜: {len(video_items)}")
            
            for idx, item in enumerate(video_items[:8]):  # ìƒìœ„ 8ê°œ
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_element = item.find_element(By.CSS_SELECTOR, "#video-title")
                    title = title_element.text.strip()
                    link = title_element.get_attribute("href")
                    
                    # ì±„ë„ëª… ì¶”ì¶œ
                    try:
                        channel_element = item.find_element(By.CSS_SELECTOR, "#channel-info #text a")
                        channel = channel_element.text.strip()
                    except:
                        channel = ""
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                    try:
                        views_element = item.find_element(By.CSS_SELECTOR, "#metadata-line span:first-child")
                        views = views_element.text.strip()
                    except:
                        views = ""
                    
                    # ì—…ë¡œë“œ ì‹œê°„ ì¶”ì¶œ
                    try:
                        time_element = item.find_element(By.CSS_SELECTOR, "#metadata-line span:last-child")
                        upload_time = time_element.text.strip()
                    except:
                        upload_time = ""
                    
                    # ì¸ë„¤ì¼ URL ì¶”ì¶œ
                    try:
                        thumbnail_element = item.find_element(By.CSS_SELECTOR, "img")
                        thumbnail = thumbnail_element.get_attribute("src")
                    except:
                        thumbnail = ""
                    
                    if not title or len(title) < 3:
                        continue
                    
                    youtube_video = {
                        "title": title,
                        "url": link,
                        "channel": channel,
                        "views": views,
                        "upload_time": upload_time,
                        "thumbnail": thumbnail,
                        "type": "youtube",
                        "keyword": keyword,
                        "search_rank": idx + 1,
                        "date": upload_time  # ì—…ë¡œë“œ ì‹œê°„ì„ ë‚ ì§œë¡œ ì‚¬ìš©
                    }
                    
                    youtube_data.append(youtube_video)
                    logger.debug(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜ì§‘: {title[:50]}...")
                    
                except Exception as e:
                    logger.warning(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            logger.info(f"ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(youtube_data)}ê°œ ë¹„ë””ì˜¤")
            return youtube_data
            
        except Exception as e:
            logger.error(f"ìœ íŠœë¸Œ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []

    def save_enhanced_data(self, data, keyword):
        """ê°œì„ ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.data_dir}/enhanced_news/{keyword}_enhanced_news_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ê°œì„ ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} ({len(data)}ê°œ í•­ëª©)")
            return filename
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return None

    def run_enhanced_crawl(self, keywords):
        """ê°œì„ ëœ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            if not self.create_webdriver():
                logger.error("ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
                return False
            
            all_data = []
            total_keywords = len(keywords)
            
            for idx, keyword in enumerate(keywords, 1):
                print(f"\nğŸ” [{idx}/{total_keywords}] í‚¤ì›Œë“œ: '{keyword}' í¬ë¡¤ë§ ì¤‘...")
                print(f"   ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ)...")
                
                # ë‰´ìŠ¤ í¬ë¡¤ë§
                news_data = self.crawl_enhanced_naver_news(keyword)
                
                print(f"   ğŸ“ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ 7ì¼, ê´€ë ¨ë„ìˆœ)...")
                # ë¸”ë¡œê·¸ í¬ë¡¤ë§
                blog_data = self.crawl_naver_blog_search(keyword)
                
                print(f"   ğŸ¥ ìœ íŠœë¸Œ ìˆ˜ì§‘ ì¤‘ (ìµœì‹ ì—…ë¡œë“œ, ê´€ë ¨ë„ìˆœ)...")
                # ìœ íŠœë¸Œ í¬ë¡¤ë§
                youtube_data = self.crawl_youtube_search(keyword)
                
                # ë°ì´í„° í•©ì¹˜ê¸°
                combined_data = news_data + blog_data + youtube_data
                
                if combined_data:
                    # í‚¤ì›Œë“œë³„ íŒŒì¼ ì €ì¥
                    self.save_enhanced_data(combined_data, keyword)
                    all_data.extend(combined_data)
                    print(f"   âœ… ì´ {len(combined_data)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ (ë‰´ìŠ¤: {len(news_data)}, ë¸”ë¡œê·¸: {len(blog_data)}, ìœ íŠœë¸Œ: {len(youtube_data)})")
                else:
                    print(f"   âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
                
                # ìš”ì²­ ê°„ ëŒ€ê¸°
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            # ì „ì²´ ìš”ì•½ ì €ì¥
            if all_data:
                summary_file = f"{self.data_dir}/enhanced_crawl_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "total_items": len(all_data),
                        "keywords": keywords,
                        "crawl_time": datetime.now().isoformat(),
                        "summary": f"{len(all_data)}ê°œ í•­ëª©ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œë¡œ ìˆ˜ì§‘ë¨"
                    }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ì „ì²´ ê°œì„ ëœ í¬ë¡¤ë§ ì™„ë£Œ: {len(all_data)}ê°œ í•­ëª©")
            return True
            
        except Exception as e:
            logger.error(f"ê°œì„ ëœ í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return False
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ") 