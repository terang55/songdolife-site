"""
ê¸°ì¡´ í¬ë¡¤ë§ëœ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ì œê±° ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import glob
from datetime import datetime
from enhanced_crawler import EnhancedNonhyeonCrawler
from loguru import logger
import config

def remove_duplicates_from_existing_data():
    """ê¸°ì¡´ í¬ë¡¤ë§ëœ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ì œê±°"""
    try:
        print("ğŸ” ê¸°ì¡´ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ì œê±°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì¤‘ë³µ ì œê±° ë©”ì„œë“œ ì‚¬ìš©ì„ ìœ„í•´)
        crawler = EnhancedNonhyeonCrawler()
        
        # enhanced_news ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸°
        enhanced_news_dir = f"{config.DATA_DIR}/enhanced_news"
        json_files = glob.glob(f"{enhanced_news_dir}/*_enhanced_news_*.json")
        
        if not json_files:
            print("âŒ ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(json_files)}ê°œ")
        
        total_before = 0
        total_after = 0
        processed_files = 0
        
        for file_path in json_files:
            try:
                filename = os.path.basename(file_path)
                print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}")
                
                # íŒŒì¼ ì½ê¸°
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if not data:
                    print("   âš ï¸ ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤.")
                    continue
                
                original_count = len(data)
                total_before += original_count
                
                # ì¤‘ë³µ ì œê±°
                unique_data = crawler.remove_duplicates(data)
                unique_count = len(unique_data)
                total_after += unique_count
                
                removed_count = original_count - unique_count
                
                if removed_count > 0:
                    # ì¤‘ë³µì´ ì œê±°ëœ ê²½ìš°ì—ë§Œ íŒŒì¼ ì—…ë°ì´íŠ¸
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(unique_data, f, ensure_ascii=False, indent=2)
                    
                    print(f"   âœ… {original_count}ê°œ â†’ {unique_count}ê°œ (ì¤‘ë³µ {removed_count}ê°œ ì œê±°)")
                    processed_files += 1
                else:
                    print(f"   âœ… {original_count}ê°œ (ì¤‘ë³µ ì—†ìŒ)")
                
            except Exception as e:
                print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ê²°ê³¼ ìš”ì•½
        total_removed = total_before - total_after
        print(f"\nğŸ“Š ì¤‘ë³µ ì œê±° ì™„ë£Œ!")
        print(f"   â€¢ ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}ê°œ")
        print(f"   â€¢ ì „ì²´ í•­ëª©: {total_before}ê°œ â†’ {total_after}ê°œ")
        print(f"   â€¢ ì œê±°ëœ ì¤‘ë³µ: {total_removed}ê°œ")
        
        if total_removed > 0:
            print(f"   â€¢ ì¤‘ë³µ ì œê±°ìœ¨: {(total_removed/total_before)*100:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"ì¤‘ë³µ ì œê±° ì˜¤ë¥˜: {str(e)}")
        return False

def remove_duplicates_across_files():
    """íŒŒì¼ ê°„ ì¤‘ë³µ ì œê±° - ëª¨ë“  íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í•©ì³ì„œ ì¤‘ë³µ ì œê±° í›„ ë‹¤ì‹œ ë¶„ë°°"""
    try:
        print("\nğŸ” íŒŒì¼ ê°„ ì¤‘ë³µ ì œê±°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        crawler = EnhancedNonhyeonCrawler()
        enhanced_news_dir = f"{config.DATA_DIR}/enhanced_news"
        json_files = glob.glob(f"{enhanced_news_dir}/*_enhanced_news_*.json")
        
        if not json_files:
            print("âŒ ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ëª¨ë“  íŒŒì¼ì˜ ë°ì´í„°ë¥¼ í‚¤ì›Œë“œë³„ë¡œ ìˆ˜ì§‘
        keyword_data = {}
        total_items = 0
        
        for file_path in json_files:
            try:
                filename = os.path.basename(file_path)
                # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                keyword = filename.split('_enhanced_news_')[0]
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if keyword not in keyword_data:
                    keyword_data[keyword] = []
                
                keyword_data[keyword].extend(data)
                total_items += len(data)
                
            except Exception as e:
                print(f"   âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({filename}): {str(e)}")
                continue
        
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {len(keyword_data)}ê°œ í‚¤ì›Œë“œ, ì´ {total_items}ê°œ í•­ëª©")
        
        # í‚¤ì›Œë“œë³„ë¡œ ì¤‘ë³µ ì œê±° ë° íŒŒì¼ ì¬ìƒì„±
        total_unique = 0
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for keyword, items in keyword_data.items():
            if not items:
                continue
            
            original_count = len(items)
            unique_items = crawler.remove_duplicates(items)
            unique_count = len(unique_items)
            removed_count = original_count - unique_count
            
            total_unique += unique_count
            
            # ìƒˆë¡œìš´ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
            new_filename = f"{enhanced_news_dir}/{keyword}_enhanced_news_{timestamp}.json"
            with open(new_filename, 'w', encoding='utf-8') as f:
                json.dump(unique_items, f, ensure_ascii=False, indent=2)
            
            if removed_count > 0:
                print(f"   âœ… {keyword}: {original_count}ê°œ â†’ {unique_count}ê°œ (ì¤‘ë³µ {removed_count}ê°œ ì œê±°)")
            else:
                print(f"   âœ… {keyword}: {unique_count}ê°œ (ì¤‘ë³µ ì—†ìŒ)")
        
        # ê¸°ì¡´ íŒŒì¼ë“¤ ì‚­ì œ (ìƒˆë¡œìš´ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì•„ë‹Œ íŒŒì¼ë“¤)
        for file_path in json_files:
            if timestamp not in file_path:
                try:
                    os.remove(file_path)
                    print(f"   ğŸ—‘ï¸ ê¸°ì¡´ íŒŒì¼ ì‚­ì œ: {os.path.basename(file_path)}")
                except:
                    pass
        
        total_removed = total_items - total_unique
        print(f"\nğŸ“Š íŒŒì¼ ê°„ ì¤‘ë³µ ì œê±° ì™„ë£Œ!")
        print(f"   â€¢ ì „ì²´ í•­ëª©: {total_items}ê°œ â†’ {total_unique}ê°œ")
        print(f"   â€¢ ì œê±°ëœ ì¤‘ë³µ: {total_removed}ê°œ")
        
        if total_removed > 0:
            print(f"   â€¢ ì¤‘ë³µ ì œê±°ìœ¨: {(total_removed/total_items)*100:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ê°„ ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜: {str(e)}")
        logger.error(f"íŒŒì¼ ê°„ ì¤‘ë³µ ì œê±° ì˜¤ë¥˜: {str(e)}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”§ ì†¡ë„ë™ ì •ë³´ í—ˆë¸Œ - ì¤‘ë³µ ì œê±° ë„êµ¬")
    print("=" * 60)
    
    print("\n1ï¸âƒ£ íŒŒì¼ ë‚´ ì¤‘ë³µ ì œê±° ì‹¤í–‰...")
    success1 = remove_duplicates_from_existing_data()
    
    if success1:
        print("\n2ï¸âƒ£ íŒŒì¼ ê°„ ì¤‘ë³µ ì œê±° ì‹¤í–‰...")
        success2 = remove_duplicates_across_files()
        
        if success2:
            print("\nğŸ‰ ëª¨ë“  ì¤‘ë³µ ì œê±° ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™”
            print("\nğŸ”„ í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì‹œì‘...")
            try:
                from sync_to_frontend import sync_data_to_frontend
                if sync_data_to_frontend():
                    print("âœ… í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì™„ë£Œ!")
                else:
                    print("âŒ í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™” ì‹¤íŒ¨!")
            except Exception as e:
                print(f"âŒ ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        else:
            print("\nâŒ íŒŒì¼ ê°„ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨!")
    else:
        print("\nâŒ íŒŒì¼ ë‚´ ì¤‘ë³µ ì œê±° ì‹¤íŒ¨!")
    
    print("\n" + "=" * 60) 