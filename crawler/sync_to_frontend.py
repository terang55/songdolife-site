"""
í¬ë¡¤ë§ ë°ì´í„°ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ìë™ ë™ê¸°í™”
í¬ë¡¤ë§ ì™„ë£Œ í›„ ìµœì‹  ë°ì´í„°ë¥¼ frontend/public/data/enhanced_news/ë¡œ ë³µì‚¬
"""

import os
import shutil
import json
import glob
from datetime import datetime
from loguru import logger
import config

class DataSyncManager:
    def __init__(self):
        """ë°ì´í„° ë™ê¸°í™” ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.source_dir = os.path.join(config.DATA_DIR, "enhanced_news")
        self.target_dir = os.path.join("../frontend/public/data/enhanced_news")
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = f"{config.LOGS_DIR}/data_sync_{datetime.now().strftime('%Y%m%d')}.log"
        os.makedirs(config.LOGS_DIR, exist_ok=True)
        
        logger.add(
            log_file,
            format=config.LOG_FORMAT,
            level=config.LOG_LEVEL,
            rotation="1 day",
            retention="30 days"
        )
        
    def ensure_target_directory(self):
        """íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±"""
        try:
            os.makedirs(self.target_dir, exist_ok=True)
            logger.info(f"íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ í™•ì¸: {self.target_dir}")
            return True
        except Exception as e:
            logger.error(f"íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    def get_all_files(self):
        """ëª¨ë“  JSON íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            all_files = {}
            
            # ëª¨ë“  JSON íŒŒì¼ ìŠ¤ìº”
            pattern = os.path.join(self.source_dir, "*_enhanced_news_*.json")
            file_paths = glob.glob(pattern)
            
            for file_path in file_paths:
                filename = os.path.basename(file_path)
                all_files[filename] = {
                    'path': file_path,
                    'filename': filename
                }
            
            logger.info(f"ì „ì²´ íŒŒì¼ {len(all_files)}ê°œ ì„ íƒ")
            return all_files
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì„ íƒ ì˜¤ë¥˜: {str(e)}")
            return {}

    def get_latest_files_by_keyword(self):
        """all_platforms í†µí•© íŒŒì¼ë§Œ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ë°©ì§€)"""
        try:
            latest_files = {}
            
            # all_platformsë¡œ ì‹œì‘í•˜ëŠ” í†µí•© íŒŒì¼ë§Œ ìŠ¤ìº”
            pattern = os.path.join(self.source_dir, "all_platforms_enhanced_news_*.json")
            all_files = glob.glob(pattern)
            
            for file_path in all_files:
                filename = os.path.basename(file_path)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ _enhanced_news_ ì´ì „ ë¶€ë¶„)
                if "_enhanced_news_" in filename:
                    keyword = filename.split("_enhanced_news_")[0]
                    timestamp = filename.split("_enhanced_news_")[1].replace(".json", "")
                    
                    # all_platforms íŒŒì¼ë§Œ ì„ íƒ (ì¤‘ë³µ ë°©ì§€)
                    if keyword == "all_platforms":
                        # í‚¤ì›Œë“œë³„ë¡œ ê°€ì¥ ìµœì‹  íŒŒì¼ë§Œ ì„ íƒ
                        if keyword not in latest_files or timestamp > latest_files[keyword]['timestamp']:
                            latest_files[keyword] = {
                                'path': file_path,
                                'timestamp': timestamp,
                                'filename': filename
                            }
            
            logger.info(f"í†µí•© íŒŒì¼ {len(latest_files)}ê°œ ì„ íƒ (ì¤‘ë³µ ë°©ì§€)")
            return latest_files
            
        except Exception as e:
            logger.error(f"ìµœì‹  íŒŒì¼ ì„ íƒ ì˜¤ë¥˜: {str(e)}")
            return {}

    def sync_all_data(self):
        """ìµœì‹  ë°ì´í„°ë§Œ í”„ë¡ íŠ¸ì—”ë“œë¡œ ë™ê¸°í™” (ì‹¤ì œë¡œëŠ” í‚¤ì›Œë“œë³„ ìµœì‹  íŒŒì¼ë§Œ)"""
        try:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ğŸ”„ ìµœì‹  ë°ì´í„° ë™ê¸°í™” ì‹œì‘...")
            
            if not self.ensure_target_directory():
                return False
                
            # ê¸°ì¡´ íŒŒì¼ë“¤ ì •ë¦¬
            self.cleanup_old_files()
            
            # í‚¤ì›Œë“œë³„ ìµœì‹  íŒŒì¼ë“¤ë§Œ ê°€ì ¸ì˜¤ê¸° (ê³¼ê±° ë°ì´í„° ì œì™¸)
            latest_files = self.get_latest_files_by_keyword()
            
            if not latest_files:
                logger.warning("ë™ê¸°í™”í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # íŒŒì¼ ë³µì‚¬
            synced_count = 0
            synced_files = {}
            for keyword, file_info in latest_files.items():
                try:
                    source_path = file_info['path']
                    target_path = os.path.join(self.target_dir, file_info['filename'])
                    
                    # íŒŒì¼ ë³µì‚¬
                    shutil.copy2(source_path, target_path)
                    synced_count += 1
                    synced_files[file_info['filename']] = file_info
                    
                    logger.debug(f"ë™ê¸°í™” ì™„ë£Œ: {keyword} -> {file_info['filename']}")
                    
                except Exception as e:
                    logger.error(f"íŒŒì¼ ë³µì‚¬ ì˜¤ë¥˜ {keyword}: {str(e)}")
                    continue
            
            logger.info(f"ìµœì‹  ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {synced_count}ê°œ íŒŒì¼")
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] âœ… ìµœì‹  ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {synced_count}ê°œ íŒŒì¼")
            
            # ë™ê¸°í™” ê²°ê³¼ ìš”ì•½ ìƒì„±
            self.create_latest_sync_summary(synced_files)
            
            return True
            
        except Exception as e:
            logger.error(f"ìµœì‹  ë°ì´í„° ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}")
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] âŒ ìµœì‹  ë°ì´í„° ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}")
            return False

    def sync_latest_data(self):
        """ìµœì‹  ë°ì´í„°ë§Œ í”„ë¡ íŠ¸ì—”ë“œë¡œ ë™ê¸°í™”"""
        try:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ğŸ”„ ë°ì´í„° ë™ê¸°í™” ì‹œì‘...")
            
            if not self.ensure_target_directory():
                return False
                
            # ê¸°ì¡´ íŒŒì¼ë“¤ ì •ë¦¬ (ì„ íƒì )
            self.cleanup_old_files()
            
            # í‚¤ì›Œë“œë³„ ìµœì‹  íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
            latest_files = self.get_latest_files_by_keyword()
            
            if not latest_files:
                logger.warning("ë™ê¸°í™”í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # íŒŒì¼ ë³µì‚¬
            synced_count = 0
            for keyword, file_info in latest_files.items():
                try:
                    source_path = file_info['path']
                    target_path = os.path.join(self.target_dir, file_info['filename'])
                    
                    # íŒŒì¼ ë³µì‚¬
                    shutil.copy2(source_path, target_path)
                    synced_count += 1
                    
                    logger.debug(f"ë™ê¸°í™” ì™„ë£Œ: {keyword} -> {file_info['filename']}")
                    
                except Exception as e:
                    logger.error(f"íŒŒì¼ ë³µì‚¬ ì˜¤ë¥˜ {keyword}: {str(e)}")
                    continue
            
            logger.info(f"ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {synced_count}ê°œ íŒŒì¼")
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] âœ… ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ: {synced_count}ê°œ íŒŒì¼")
            
            # ë™ê¸°í™” ê²°ê³¼ ìš”ì•½ ìƒì„±
            self.create_sync_summary(latest_files)
            
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}")
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] âŒ ë°ì´í„° ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}")
            return False

    def cleanup_old_files(self, keep_days=7):
        """ê¸°ì¡´ íŒŒì¼ë“¤ ì •ë¦¬"""
        try:
            # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ ì‚­ì œ (sync_summary.json ì œì™¸)
            pattern = os.path.join(self.target_dir, "*_enhanced_news_*.json")
            existing_files = glob.glob(pattern)
            
            deleted_count = 0
            for file_path in existing_files:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                    logger.debug(f"ê¸°ì¡´ íŒŒì¼ ì‚­ì œ: {os.path.basename(file_path)}")
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file_path}: {str(e)}")
            
            if deleted_count > 0:
                logger.info(f"ê¸°ì¡´ íŒŒì¼ {deleted_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"íŒŒì¼ ì •ë¦¬ ì˜¤ë¥˜: {str(e)}")

    def create_latest_sync_summary(self, synced_files):
        """ìµœì‹  ë°ì´í„° ë™ê¸°í™” ìš”ì•½ ì •ë³´ ìƒì„±"""
        try:
            # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
            unique_keywords = set()
            
            # all_platforms íŒŒì¼ì—ì„œ ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ
            for filename in synced_files.keys():
                if "all_platforms" in filename:
                    try:
                        file_path = os.path.join(self.target_dir, filename)
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            for item in data:
                                if 'keyword' in item:
                                    unique_keywords.add(item['keyword'])
                    except Exception as e:
                        logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            # ì‹¤ì œ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬)
            actual_keywords = sorted(list(unique_keywords))
            
            summary = {
                "sync_time": datetime.now().isoformat(),
                "total_files": len(synced_files),
                "total_keywords": len(actual_keywords),  # ì‹¤ì œ í‚¤ì›Œë“œ ìˆ˜
                "keywords": actual_keywords,  # ì‹¤ì œ ê²€ìƒ‰ í‚¤ì›Œë“œë“¤
                "file_types": list(synced_files.keys()),  # íŒŒì¼ íƒ€ì…ë“¤
                "files": {k: k for k in synced_files.keys()},
                "sync_type": "latest_only"
            }
            
            summary_path = os.path.join(self.target_dir, "sync_summary.json")
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ìµœì‹  ë°ì´í„° ë™ê¸°í™” ìš”ì•½ ìƒì„±: {summary_path} (ì‹¤ì œ í‚¤ì›Œë“œ {len(actual_keywords)}ê°œ)")
            
        except Exception as e:
            logger.warning(f"ìµœì‹  ë°ì´í„° ë™ê¸°í™” ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")

    def create_all_sync_summary(self, synced_files):
        """ì „ì²´ ë™ê¸°í™” ìš”ì•½ ì •ë³´ ìƒì„±"""
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = set()
            for filename in synced_files.keys():
                if "_enhanced_news_" in filename:
                    keyword = filename.split("_enhanced_news_")[0]
                    keywords.add(keyword)
            
            summary = {
                "sync_time": datetime.now().isoformat(),
                "total_files": len(synced_files),
                "keywords": list(keywords),
                "files": {k: v['filename'] for k, v in synced_files.items()},
                "sync_type": "all_files"
            }
            
            summary_path = os.path.join(self.target_dir, "sync_summary.json")
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ì „ì²´ ë™ê¸°í™” ìš”ì•½ ìƒì„±: {summary_path}")
            
        except Exception as e:
            logger.warning(f"ì „ì²´ ë™ê¸°í™” ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")

    def create_sync_summary(self, synced_files):
        """ë™ê¸°í™” ìš”ì•½ ì •ë³´ ìƒì„±"""
        try:
            # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
            unique_keywords = set()
            
            # all_platforms íŒŒì¼ì—ì„œ ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ
            for keyword, file_info in synced_files.items():
                if "all_platforms" in keyword:
                    try:
                        source_path = file_info['path']
                        with open(source_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            for item in data:
                                if 'keyword' in item:
                                    unique_keywords.add(item['keyword'])
                    except Exception as e:
                        logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            # ì‹¤ì œ ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬)
            actual_keywords = sorted(list(unique_keywords))
            
            summary = {
                "sync_time": datetime.now().isoformat(),
                "total_files": len(synced_files),
                "total_keywords": len(actual_keywords),  # ì‹¤ì œ í‚¤ì›Œë“œ ìˆ˜
                "keywords": actual_keywords,  # ì‹¤ì œ ê²€ìƒ‰ í‚¤ì›Œë“œë“¤
                "file_types": list(synced_files.keys()),  # íŒŒì¼ íƒ€ì…ë“¤ 
                "files": {k: v['filename'] for k, v in synced_files.items()},
                "sync_type": "latest_only"
            }
            
            summary_path = os.path.join(self.target_dir, "sync_summary.json")
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ë™ê¸°í™” ìš”ì•½ ìƒì„±: {summary_path} (ì‹¤ì œ í‚¤ì›Œë“œ {len(actual_keywords)}ê°œ)")
            
        except Exception as e:
            logger.warning(f"ë™ê¸°í™” ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")

    def get_sync_status(self):
        """ë™ê¸°í™” ìƒíƒœ í™•ì¸"""
        try:
            summary_path = os.path.join(self.target_dir, "sync_summary.json")
            
            if os.path.exists(summary_path):
                with open(summary_path, 'r', encoding='utf-8') as f:
                    summary = json.load(f)
                return summary
            else:
                return {"error": "ë™ê¸°í™” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"}
                
        except Exception as e:
            return {"error": f"ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}"}

def sync_data_to_frontend():
    """ë©”ì¸ ë™ê¸°í™” í•¨ìˆ˜ (ìµœì‹  íŒŒì¼ë§Œ)"""
    sync_manager = DataSyncManager()
    return sync_manager.sync_latest_data()

def sync_all_data_to_frontend():
    """ì „ì²´ ë°ì´í„° ë™ê¸°í™” í•¨ìˆ˜"""
    sync_manager = DataSyncManager()
    return sync_manager.sync_all_data()

def check_sync_status():
    """ë™ê¸°í™” ìƒíƒœ í™•ì¸"""
    sync_manager = DataSyncManager()
    status = sync_manager.get_sync_status()
    
    print("ğŸ“Š ë°ì´í„° ë™ê¸°í™” ìƒíƒœ:")
    print("=" * 40)
    
    if "error" in status:
        print(f"âŒ {status['error']}")
    else:
        print(f"ğŸ• ë§ˆì§€ë§‰ ë™ê¸°í™”: {status.get('sync_time', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
        print(f"ğŸ“ ë™ê¸°í™”ëœ íŒŒì¼ ìˆ˜: {status.get('total_files', 0)}ê°œ")
        print(f"ğŸ·ï¸ í‚¤ì›Œë“œ: {', '.join(status.get('keywords', []))}")
        print(f"ğŸ“„ ë™ê¸°í™” íƒ€ì…: {status.get('sync_type', 'ì•Œ ìˆ˜ ì—†ìŒ')}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--sync":
            print("ğŸ”„ ìµœì‹  ë°ì´í„° ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            success = sync_data_to_frontend()
            if success:
                print("âœ… ë™ê¸°í™” ì™„ë£Œ!")
            else:
                print("âŒ ë™ê¸°í™” ì‹¤íŒ¨!")
        elif sys.argv[1] == "--sync-all":
            print("ğŸ”„ ì „ì²´ ë°ì´í„° ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            success = sync_all_data_to_frontend()
            if success:
                print("âœ… ì „ì²´ ë™ê¸°í™” ì™„ë£Œ!")
            else:
                print("âŒ ì „ì²´ ë™ê¸°í™” ì‹¤íŒ¨!")
        elif sys.argv[1] == "--status":
            check_sync_status()
        else:
            print("ì‚¬ìš©ë²•: python sync_to_frontend.py [--sync|--sync-all|--status]")
    else:
        print("ë…¼í˜„ë™ ì •ë³´ í—ˆë¸Œ - ë°ì´í„° ë™ê¸°í™” ë„êµ¬")
        print("=" * 40)
        print("1. ìµœì‹  ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰")
        print("2. ì „ì²´ ë°ì´í„° ë™ê¸°í™” ì‹¤í–‰")
        print("3. ë™ê¸°í™” ìƒíƒœ í™•ì¸")
        print("4. ì¢…ë£Œ")
        
        choice = input("ì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
        
        if choice == "1":
            print("ğŸ”„ ìµœì‹  ë°ì´í„° ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            success = sync_data_to_frontend()
            if success:
                print("âœ… ë™ê¸°í™” ì™„ë£Œ!")
            else:
                print("âŒ ë™ê¸°í™” ì‹¤íŒ¨!")
        elif choice == "2":
            print("ğŸ”„ ì „ì²´ ë°ì´í„° ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            success = sync_all_data_to_frontend()
            if success:
                print("âœ… ì „ì²´ ë™ê¸°í™” ì™„ë£Œ!")
            else:
                print("âŒ ì „ì²´ ë™ê¸°í™” ì‹¤íŒ¨!")
        elif choice == "3":
            check_sync_status()
        elif choice == "4":
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 