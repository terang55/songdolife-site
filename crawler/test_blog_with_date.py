#!/usr/bin/env python3
"""
ë‚ ì§œ ì •ë³´ í¬í•¨ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_crawler import EnhancedNonhyeonCrawler
import json

def test_blog_with_date():
    """ë¸”ë¡œê·¸ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== ë‚ ì§œ ì •ë³´ í¬í•¨ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===")
    
    crawler = EnhancedNonhyeonCrawler()
    
    # ì›¹ë“œë¼ì´ë²„ ìƒì„±
    if not crawler.create_webdriver():
        print("âŒ ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
        return
    
    try:
        # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ
        test_keyword = "ì¸ì²œ ë…¼í˜„ë™"
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ: {test_keyword}")
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§
        blog_data = crawler.crawl_naver_blog_search(test_keyword)
        
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(blog_data)}ê°œ ë¸”ë¡œê·¸")
        
        for i, blog in enumerate(blog_data, 1):
            print(f"\nğŸ“ ë¸”ë¡œê·¸ {i}:")
            print(f"   ì œëª©: {blog['title'][:60]}...")
            print(f"   ë¸”ë¡œê·¸ëª…: {blog['source']}")
            print(f"   ë‚ ì§œ: {blog['date']}")
            print(f"   URL: {blog['url'][:50]}...")
            print(f"   ë‚´ìš©: {blog['content'][:80]}...")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        with open("test_blog_with_date.json", "w", encoding="utf-8") as f:
            json.dump(blog_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ê°€ test_blog_with_date.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
    finally:
        try:
            crawler.driver.quit()
        except:
            pass

if __name__ == "__main__":
    test_blog_with_date() 