#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

from enhanced_crawler import EnhancedNonhyeonCrawler
import json

def test_cafe_crawling():
    """ì¸ì²œ ë…¼í˜„ë™ ì¹´í˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("=== ë„¤ì´ë²„ ì¹´í˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = EnhancedNonhyeonCrawler()
    
    try:
        # ì›¹ë“œë¼ì´ë²„ ìƒì„±
        if not crawler.create_webdriver():
            print("âŒ ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            return
        
        # "ì¸ì²œ ë…¼í˜„ë™" ì¹´í˜ í¬ë¡¤ë§
        keyword = "ì¸ì²œ ë…¼í˜„ë™"
        print(f"ğŸ” '{keyword}' ì¹´í˜ í¬ë¡¤ë§ ì‹œì‘...")
        
        cafe_data = crawler.crawl_naver_cafe_search(keyword)
        
        print(f"\nâœ… ìˆ˜ì§‘ëœ ì¹´í˜ ê²Œì‹œê¸€: {len(cafe_data)}ê°œ")
        print("=" * 60)
        
        # ê²°ê³¼ ì¶œë ¥
        for i, post in enumerate(cafe_data, 1):
            print(f"\n{i}. ì œëª©: {post['title']}")
            print(f"   ì¹´í˜: {post['source']}")
            print(f"   ì‘ì„±ì: {post['author']}")
            print(f"   ì‘ì„±ì¼: {post['date']}")
            print(f"   URL: {post['url']}")
            print(f"   ë‚´ìš©: {post['content'][:100]}...")
            print(f"   ê¸¸ì´: {post['content_length']}ì")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = f"test_cafe_{keyword.replace(' ', '_')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(cafe_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë‹¤ë¥¸ í‚¤ì›Œë“œë„ í…ŒìŠ¤íŠ¸
        test_keywords = ["ë…¼í˜„ë™", "ë…¼í˜„ì§€êµ¬", "ì¸ì²œ ë‚¨ë™êµ¬"]
        
        for test_keyword in test_keywords:
            print(f"\nğŸ” '{test_keyword}' ì¹´í˜ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸...")
            test_data = crawler.crawl_naver_cafe_search(test_keyword)
            print(f"   âœ… '{test_keyword}': {len(test_data)}ê°œ ìˆ˜ì§‘")
            
            # ê° í‚¤ì›Œë“œë³„ë¡œ íŒŒì¼ ì €ì¥
            test_output_file = f"test_cafe_{test_keyword.replace(' ', '_')}.json"
            with open(test_output_file, 'w', encoding='utf-8') as f:
                json.dump(test_data, f, ensure_ascii=False, indent=2)
            print(f"   ğŸ’¾ '{test_output_file}' ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    finally:
        # ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ
        if crawler.driver:
            crawler.driver.quit()
            print("\nğŸ”š ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")

if __name__ == "__main__":
    test_cafe_crawling() 