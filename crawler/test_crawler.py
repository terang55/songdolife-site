"""
ë…¼í˜„ë™ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ê°„ë‹¨í•œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸
"""

from main_crawler import NonhyeonCrawler
import config

def test_news_crawling():
    """ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    crawler = NonhyeonCrawler()
    
    # ì›¹ë“œë¼ì´ë²„ ìƒì„± í…ŒìŠ¤íŠ¸
    print("1ï¸âƒ£ ì›¹ë“œë¼ì´ë²„ ìƒì„± í…ŒìŠ¤íŠ¸...")
    if not crawler.create_webdriver():
        print("âŒ ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
        return False
    
    print("âœ… ì›¹ë“œë¼ì´ë²„ ìƒì„± ì„±ê³µ")
    
    try:
        # ë‹¨ì¼ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
        print("2ï¸âƒ£ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ (í‚¤ì›Œë“œ: ë…¼í˜„ë™)...")
        news_data = crawler.crawl_naver_news("ë…¼í˜„ë™")
        
        if news_data:
            print(f"âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì„±ê³µ: {len(news_data)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            
            # ì²« ë²ˆì§¸ ê¸°ì‚¬ ì •ë³´ ì¶œë ¥
            if len(news_data) > 0:
                print("ğŸ“° ì²« ë²ˆì§¸ ê¸°ì‚¬ ìƒ˜í”Œ:")
                first_article = news_data[0]
                print(f"   ì œëª©: {first_article.get('title', 'ì œëª© ì—†ìŒ')}")
                print(f"   ì–¸ë¡ ì‚¬: {first_article.get('press', 'ì–¸ë¡ ì‚¬ ì—†ìŒ')}")
                print(f"   ë‚ ì§œ: {first_article.get('date', 'ë‚ ì§œ ì—†ìŒ')}")
                
            # ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸
            print("3ï¸âƒ£ ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸...")
            save_result = crawler.save_data(news_data, "news", "test_ë…¼í˜„ë™")
            if save_result:
                print(f"âœ… ë°ì´í„° ì €ì¥ ì„±ê³µ: {save_result}")
            else:
                print("âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
        else:
            print("âŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")
            return False
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False
        
    finally:
        if crawler.driver:
            crawler.driver.quit()
            print("ğŸ”š ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ")
    
    return True

def test_data_directory():
    """ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬ í…ŒìŠ¤íŠ¸...")
    
    crawler = NonhyeonCrawler()
    data_dir = crawler.ensure_data_directory()
    
    import os
    required_dirs = [
        config.DATA_DIR,
        f"{config.DATA_DIR}/news",
        f"{config.DATA_DIR}/cafe", 
        f"{config.DATA_DIR}/community"
    ]
    
    all_exist = True
    for dir_path in required_dirs:
        if os.path.exists(dir_path):
            print(f"âœ… {dir_path}")
        else:
            print(f"âŒ {dir_path} - ìƒì„±ë˜ì§€ ì•ŠìŒ")
            all_exist = False
    
    return all_exist

def main():
    """í…ŒìŠ¤íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ§ª ë…¼í˜„ë™ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ 1: ë°ì´í„° ë””ë ‰í† ë¦¬
    test1_result = test_data_directory()
    print()
    
    # í…ŒìŠ¤íŠ¸ 2: ë‰´ìŠ¤ í¬ë¡¤ë§
    test2_result = test_news_crawling()
    print()
    
    # ê²°ê³¼ ì¶œë ¥
    print("=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {'âœ… í†µê³¼' if test1_result else 'âŒ ì‹¤íŒ¨'}")
    print(f"ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§: {'âœ… í†µê³¼' if test2_result else 'âŒ ì‹¤íŒ¨'}")
    
    if test1_result and test2_result:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! í¬ë¡¤ëŸ¬ê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë¬¸ì œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main() 