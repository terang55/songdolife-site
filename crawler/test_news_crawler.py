#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

from enhanced_crawler import EnhancedNonhyeonCrawler
import json

def test_news_crawling():
    """ì¸ì²œ ë…¼í˜„ë™ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("=== ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===")
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = EnhancedNonhyeonCrawler()
    
    try:
        # ì›¹ë“œë¼ì´ë²„ ìƒì„±
        if not crawler.create_webdriver():
            print("âŒ ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            return
        
        # "ì¸ì²œ ë…¼í˜„ë™" ë‰´ìŠ¤ í¬ë¡¤ë§
        keyword = "ì¸ì²œ ë…¼í˜„ë™"
        print(f"ğŸ” '{keyword}' ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        
        news_data = crawler.crawl_enhanced_naver_news(keyword)
        
        print(f"\nâœ… ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news_data)}ê°œ")
        print("=" * 60)
        
        # ê²°ê³¼ ì¶œë ¥
        for i, news in enumerate(news_data, 1):
            print(f"\n{i}. ì œëª©: {news['title']}")
            print(f"   ì–¸ë¡ ì‚¬: {news['press']}")
            print(f"   URL: {news['url']}")
            print(f"   ìš”ì•½: {news['content'][:100]}...")
            print(f"   ê¸¸ì´: {news['content_length']}ì")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = f"test_news_{keyword.replace(' ', '_')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    finally:
        # ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ
        if crawler.driver:
            crawler.driver.quit()
            print("\nğŸ”š ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")

if __name__ == "__main__":
    test_news_crawling() 