#!/usr/bin/env python3
"""
ë‚ ì§œ ì •ë³´ í¬í•¨ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_crawler import EnhancedNonhyeonCrawler
import json

def test_news_with_date():
    """ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== ë‚ ì§œ ì •ë³´ í¬í•¨ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ===")
    
    crawler = EnhancedNonhyeonCrawler()
    
    # ì›¹ë“œë¼ì´ë²„ ìƒì„±
    if not crawler.create_webdriver():
        print("âŒ ì›¹ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
        return
    
    try:
        # í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ
        test_keyword = "ì¸ì²œ ë…¼í˜„ë™"
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ í‚¤ì›Œë“œ: {test_keyword}")
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
        news_data = crawler._crawl_naver_news_search(test_keyword)
        
        print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(news_data)}ê°œ ë‰´ìŠ¤")
        
        for i, news in enumerate(news_data, 1):
            print(f"\nğŸ“° ë‰´ìŠ¤ {i}:")
            print(f"   ì œëª©: {news.get('title', 'N/A')}")
            print(f"   ì–¸ë¡ ì‚¬: {news.get('press', 'N/A')}")
            print(f"   ğŸ“… ë‚ ì§œ: {news.get('date', 'N/A')}")  # ë‚ ì§œ ì •ë³´ í™•ì¸
            print(f"   URL: {news.get('url', 'N/A')[:80]}...")
            print(f"   ìš”ì•½: {news.get('summary', 'N/A')[:100]}...")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥í•´ì„œ í™•ì¸
        output_file = f"test_news_with_date_{test_keyword.replace(' ', '_')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ë¥¼ {output_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        if crawler.driver:
            crawler.driver.quit()

if __name__ == "__main__":
    test_news_with_date() 